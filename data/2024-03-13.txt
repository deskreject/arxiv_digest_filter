\\
arXiv:2403.06995
Date: Sun, 3 Mar 2024 07:50:29 GMT   (738kb)

Title: Exact algorithms and heuristics for capacitated covering salesman
  problems
Authors: Lucas Porto Maziero, F\'abio Luiz Usberti, Celso Cavellucci
Categories: cs.AI
\\
  This paper introduces the Capacitated Covering Salesman Problem (CCSP),
approaching the notion of service by coverage in capacitated vehicle routing
problems. In CCSP, locations where vehicles can transit are provided, some of
which have customers with demands. The objective is to service customers
through a fleet of vehicles based in a depot, minimizing the total distance
traversed by the vehicles. CCSP is unique in the sense that customers, to be
serviced, do not need to be visited by a vehicle. Instead, they can be serviced
if they are within a coverage area of the vehicle. This assumption is motivated
by applications in which some customers are unreachable (e.g., forbidden access
to vehicles) or visiting every customer is impractical. In this work,
optimization methodologies are proposed for the CCSP based on ILP (Integer
Linear Programming) and BRKGA (Biased Random-Key Genetic Algorithm)
metaheuristic. Computational experiments conducted on a benchmark of instances
for the CCSP evaluate the performance of the methodologies with respect to
primal bounds. Furthermore, our ILP formulation is extended in order to create
a novel MILP (Mixed Integer Linear Programming) for the Multi-Depot Covering
Tour Vehicle Routing Problem (MDCTVRP). Computational experiments show that the
extended MILP formulation outperformed the previous state-of-the-art exact
approach with respect to optimality gaps. In particular, optimal solutions were
obtained for several previously unsolved instances.
\\ ( https://arxiv.org/abs/2403.06995 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06996
Date: Sun, 3 Mar 2024 10:38:57 GMT   (27634kb,D)

Title: On the stochastics of human and artificial creativity
Authors: Solve S{\ae}b{\o} and Helge Brovold
Categories: cs.AI
Comments: 40 pages, 1 figure with 2 sub-figures
MSC-class: 34, 37, 60, 62
ACM-class: G.1.7; G.3; I.2; J.4; J.5
\\
  What constitutes human creativity, and is it possible for computers to
exhibit genuine creativity? We argue that achieving human-level intelligence in
computers, or so-called Artificial General Intelligence, necessitates attaining
also human-level creativity. We contribute to this discussion by developing a
statistical representation of human creativity, incorporating prior insights
from stochastic theory, psychology, philosophy, neuroscience, and chaos theory.
This highlights the stochastic nature of the human creative process, which
includes both a bias guided, random proposal step, and an evaluation step
depending on a flexible or transformable bias structure. The acquired
representation of human creativity is subsequently used to assess the
creativity levels of various contemporary AI systems. Our analysis includes
modern AI algorithms such as reinforcement learning, diffusion models, and
large language models, addressing to what extent they measure up to human level
creativity. We conclude that these technologies currently lack the capability
for autonomous creative action at a human level.
\\ ( https://arxiv.org/abs/2403.06996 ,  27634kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07003
Date: Thu, 7 Mar 2024 12:10:19 GMT   (26469kb,D)

Title: Evacuation Management Framework towards Smart City-wide Intelligent
  Emergency Interactive Response System
Authors: Anuj Abraham and Yi Zhang and Shitala Prasad
Categories: cs.AI cs.CY cs.LG cs.NI
\\
  A smart city solution toward future 6G network deployment allows small and
medium sized enterprises (SMEs), industry, and government entities to connect
with the infrastructures and play a crucial role in enhancing emergency
preparedness with advanced sensors. The objective of this work is to propose a
set of coordinated technological solutions to transform an existing emergency
response system into an intelligent interactive system, thereby improving the
public services and the quality of life for residents at home, on road, in
hospitals, transport hubs, etc. In this context, we consider a city wide view
from three different application scenes that are closely related to peoples
daily life, to optimize the actions taken at relevant departments. Therefore,
using artificial intelligence (AI) and machine learning (ML) techniques to
enable the next generation connected vehicle experiences, we specifically focus
on accidents happening in indoor households, urban roads, and at large public
facilities. This smart interactive response system will benefit from advanced
sensor fusion and AI by formulating a real time dynamic model.
\\ ( https://arxiv.org/abs/2403.07003 ,  26469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07004
Date: Thu, 7 Mar 2024 13:14:21 GMT   (129kb)

Title: Convergence of Some Convex Message Passing Algorithms to a Fixed Point
Authors: Vaclav Voracek, Tomas Werner
Categories: cs.AI cs.LG math.OC stat.ML
\\
  A popular approach to the MAP inference problem in graphical models is to
minimize an upper bound obtained from a dual linear programming or Lagrangian
relaxation by (block-)coordinate descent. Examples of such algorithms are
max-sum diffusion and sequential tree-reweighted message passing. Convergence
properties of these methods are currently not fully understood. They have been
proved to converge to the set characterized by local consistency of active
constraints, with unknown convergence rate; however, it was not clear if the
iterates converge at all (to any single point). We prove a stronger result
(which was conjectured before but never proved): the iterates converge to a
fixed point of the algorithm. Moreover, we show that they achieve precision
$\varepsilon>0$ in $\mathcal{O}(1/\varepsilon)$ iterations.
  We first prove this for a version of coordinate descent applied to a general
piecewise-affine convex objective, using a novel proof technique. Then we
demonstrate the generality of this approach by reducing some popular
coordinate-descent algorithms to this problem. Finally we show that, in
contrast to our main result, a similar version of coordinate descent applied to
a constrained optimization problem need not converge.
\\ ( https://arxiv.org/abs/2403.07004 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07005
Date: Fri, 8 Mar 2024 06:38:22 GMT   (2049kb,D)

Title: Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines
Authors: Xuejing Zheng, Chao Yu
Categories: cs.AI cs.LG cs.MA
\\
  In this paper, we study the cooperative Multi-Agent Reinforcement Learning
(MARL) problems using Reward Machines (RMs) to specify the reward functions
such that the prior knowledge of high-level events in a task can be leveraged
to facilitate the learning efficiency. Unlike the existing work that RMs have
been incorporated into MARL for task decomposition and policy learning in
relatively simple domains or with an assumption of independencies among the
agents, we present Multi-Agent Reinforcement Learning with a Hierarchy of RMs
(MAHRM) that is capable of dealing with more complex scenarios when the events
among agents can occur concurrently and the agents are highly interdependent.
  MAHRM exploits the relationship of high-level events to decompose a task into
a hierarchy of simpler subtasks that are assigned to a small group of agents,
so as to reduce the overall computational complexity.
  Experimental results in three cooperative MARL domains show that MAHRM
outperforms other MARL methods using the same prior knowledge of high-level
events.
\\ ( https://arxiv.org/abs/2403.07005 ,  2049kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07010
Date: Sat, 9 Mar 2024 04:19:50 GMT   (767kb)

Title: On Globular T-Spherical Fuzzy (G-TSF) Sets with Application to G-TSF
  Multi-Criteria Group Decision-Making
Authors: Miin-Shen Yang, Yasir Akhtar, Mehboob Ali
Categories: cs.AI
\\
  In this paper, we give the concept of Globular T-Spherical Fuzzy (G-TSF) Sets
(G-TSFSs) as an innovative extension of T-Spherical Fuzzy Sets (TSFSs) and
Circular Spherical Fuzzy Sets (C-SFSs). G-TSFSs represent membership,
indeterminacy, and non-membership degrees using a globular/sphere bound that
can offer a more accurate portrayal of vague, ambiguous, and imprecise
information. By employing a structured representation of data points on a
sphere with a specific center and radius, this model enhances decision-making
processes by enabling a more comprehensive evaluation of objects within a
flexible region. Following the newly defined G-TSFSs, we establish some basic
set operations and introduce fundamental algebraic operations for G-TSF Values
(G-TSFVs). These operations expand the evaluative capabilities of
decision-makers, facilitating more sensitive decision-making processes in a
broader region. To quantify a similarity measure (SM) between GTSFVs, the SM is
defined based on the radius of G-TSFSs. Additionally, Hamming distance and
Euclidean distance are introduced for G-TSFSs. We also present theorems and
examples to elucidate computational mechanisms. Furthermore, we give the G-TSF
Weighted Average (G-TSFWA) and G-TSF Weighted Geometric (G-TSFWG) operators.
Leveraging our proposed SM, a Multi-Criteria Group Decision-Making (MCGDM)
scheme for G-TSFSs, named G-TSF MCGDM (G-TSFMCGDM), is developed to address
group decision-making problems. The applicability and effectiveness of the
proposed G-TSFMCGDM method are demonstrated by applying it to solve the
selection problem of the best venue for professional development training
sessions in a firm. The analysis results affirm the suitability and utility of
the proposed method for resolving MCGDM problems, establishing its
effectiveness in practical decision-making scenarios.
\\ ( https://arxiv.org/abs/2403.07010 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07131
Date: Mon, 11 Mar 2024 19:55:08 GMT   (881kb,D)

Title: Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot
  Task Allocation
Authors: Steve Paul, Nathan Maurer, Souma Chowdhury
Categories: cs.AI cs.MA
Comments: This paper was accepted for presentation in proceedings of IEEE
  International Conference on Robotics and Automation 2024
\\
  Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and
efficient decision-making, which is often achieved using heuristics-aided
methods such as genetic algorithms, auction-based methods, and bipartite graph
matching methods. These methods often assume a form that lends better
explainability compared to an end-to-end (learnt) neural network based policy
for MRTA. However, deriving suitable heuristics can be tedious, risky and in
some cases impractical if problems are too complex. This raises the question:
can these heuristics be learned? To this end, this paper particularly develops
a Graph Reinforcement Learning (GRL) framework to learn the heuristics or
incentives for a bipartite graph matching approach to MRTA. Specifically a
Capsule Attention policy model is used to learn how to weight task/robot
pairings (edges) in the bipartite graph that connects the set of tasks to the
set of robots. The original capsule attention network architecture is
fundamentally modified by adding encoding of robots' state graph, and two
Multihead Attention based decoders whose output are used to construct a
LogNormal distribution matrix from which positive bigraph weights can be drawn.
The performance of this new bigraph matching approach augmented with a
GRL-derived incentive is found to be at par with the original bigraph matching
approach that used expert-specified heuristics, with the former offering
notable robustness benefits. During training, the learned incentive policy is
found to get initially closer to the expert-specified incentive and then
slightly deviate from its trend.
\\ ( https://arxiv.org/abs/2403.07131 ,  881kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07363
Date: Tue, 12 Mar 2024 06:52:24 GMT   (860kb)

Title: A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees
Authors: Yingtao Ren, Xiaomin Zhu, Kaiyuan Bai, Runtong Zhang
Categories: cs.AI
Journal-ref: IEEE Transactions on Fuzzy Systems 31.5 (2023): 1729-1741
DOI: 10.1109/TFUZZ.2022.3215725
\\
  Classification is essential to the applications in the field of data mining,
artificial intelligence, and fault detection. There exists a strong need in
developing accurate, suitable, and efficient classification methods and
algorithms with broad applicability. Random forest is a general algorithm that
is often used for classification under complex conditions. Although it has been
widely adopted, its combination with diverse fuzzy theory is still worth
exploring. In this paper, we propose the intuitionistic fuzzy random forest
(IFRF), a new random forest ensemble of intuitionistic fuzzy decision trees
(IFDT). Such trees in forest use intuitionistic fuzzy information gain to
select features and consider hesitation in information transmission. The
proposed method enjoys the power of the randomness from bootstrapped sampling
and feature selection, the flexibility of fuzzy logic and fuzzy sets, and the
robustness of multiple classifier systems. Extensive experiments demonstrate
that the IFRF has competitative and superior performance compared to other
state-of-the-art fuzzy and ensemble algorithms. IFDT is more suitable for
ensemble learning with outstanding classification accuracy. This study is the
first to propose a random forest ensemble based on the intuitionistic fuzzy
theory.
\\ ( https://arxiv.org/abs/2403.07363 ,  860kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07510
Date: Tue, 12 Mar 2024 10:45:45 GMT   (490kb,D)

Title: Relevance Score: A Landmark-Like Heuristic for Planning
Authors: Oliver Kim and Mohan Sridharan
Categories: cs.AI
Comments: 12 Pages, 3 figures
ACM-class: I.2.8
\\
  Landmarks are facts or actions that appear in all valid solutions of a
planning problem. They have been used successfully to calculate heuristics that
guide the search for a plan. We investigate an extension to this concept by
defining a novel "relevance score" that helps identify facts or actions that
appear in most but not all plans to achieve any given goal. We describe an
approach to compute this relevance score and use it as a heuristic in the
search for a plan. We experimentally compare the performance of our approach
with that of a state of the art landmark-based heuristic planning approach
using benchmark planning problems. While the original landmark-based heuristic
leads to better performance on problems with well-defined landmarks, our
approach substantially improves performance on problems that lack non-trivial
landmarks.
\\ ( https://arxiv.org/abs/2403.07510 ,  490kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07548
Date: Tue, 12 Mar 2024 11:33:48 GMT   (12912kb,D)

Title: Online Continual Learning For Interactive Instruction Following Agents
Authors: Byeonghwi Kim, Minhyuk Seo, Jonghyun Choi
Categories: cs.AI cs.LG cs.RO
Comments: ICLR 2024 (Project page:
  $\href{https://bhkim94.github.io/projects/CL-ALFRED>}{\text{https}}$)
\\
  In learning an embodied agent executing daily tasks via language directives,
the literature largely assumes that the agent learns all training data at the
beginning. We argue that such a learning scenario is less realistic since a
robotic agent is supposed to learn the world continuously as it explores and
perceives it. To take a step towards a more realistic embodied agent learning
scenario, we propose two continual learning setups for embodied agents;
learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new
environments (Environment Incremental Learning, Environment-IL) For the tasks,
previous 'data prior' based continual learning methods maintain logits for the
past tasks. However, the stored information is often insufficiently learned
information and requires task boundary information, which might not always be
available. Here, we propose to update them based on confidence scores without
task boundary information during training (i.e., task-free) in a moving average
fashion, named Confidence-Aware Moving Average (CAMA). In the proposed
Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior state
of the art in our empirical validations by noticeable margins. The project page
including codes is https://github.com/snumprlab/cl-alfred.
\\ ( https://arxiv.org/abs/2403.07548 ,  12912kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07566
Date: Tue, 12 Mar 2024 11:53:00 GMT   (298kb,D)

Title: An Improved Strategy for Blood Glucose Control Using Multi-Step Deep
  Reinforcement Learning
Authors: Weiwei Gu and Senquan Wang
Categories: cs.AI
\\
  Blood Glucose (BG) control involves keeping an individual's BG within a
healthy range through extracorporeal insulin injections is an important task
for people with type 1 diabetes. However,traditional patient self-management is
cumbersome and risky. Recent research has been devoted to exploring
individualized and automated BG control approaches, among which Deep
Reinforcement Learning (DRL) shows potential as an emerging approach. In this
paper, we use an exponential decay model of drug concentration to convert the
formalization of the BG control problem, which takes into account the delay and
prolongedness of drug effects, from a PAE-POMDP (Prolonged Action
Effect-Partially Observable Markov Decision Process) to a MDP, and we propose a
novel multi-step DRL-based algorithm to solve the problem. The Prioritized
Experience Replay (PER) sampling method is also used in it. Compared to
single-step bootstrapped updates, multi-step learning is more efficient and
reduces the influence from biasing targets. Our proposed method converges
faster and achieves higher cumulative rewards compared to the benchmark in the
same training environment, and improves the time-in-range (TIR), the percentage
of time the patient's BG is within the target range, in the evaluation phase.
Our work validates the effectiveness of multi-step reinforcement learning in BG
control, which may help to explore the optimal glycemic control measure and
improve the survival of diabetic patients.
\\ ( https://arxiv.org/abs/2403.07566 ,  298kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07587
Date: Tue, 12 Mar 2024 12:18:20 GMT   (2171kb,D)

Title: Perennial Semantic Data Terms of Use for Decentralized Web
Authors: Rui Zhao, Jun Zhao
Categories: cs.AI cs.CY cs.LO
Comments: This paper is accepted by International World Wide Web Conference
  2024 (WWW 2024 / The Web Conf 2024)
DOI: 10.1145/3589334.3645631
\\
  In today's digital landscape, the Web has become increasingly centralized,
raising concerns about user privacy violations. Decentralized Web
architectures, such as Solid, offer a promising solution by empowering users
with better control over their data in their personal `Pods'. However, a
significant challenge remains: users must navigate numerous applications to
decide which application can be trusted with access to their data Pods. This
often involves reading lengthy and complex Terms of Use agreements, a process
that users often find daunting or simply ignore. This compromises user autonomy
and impedes detection of data misuse. We propose a novel formal description of
Data Terms of Use (DToU), along with a DToU reasoner. Users and applications
specify their own parts of the DToU policy with local knowledge, covering
permissions, requirements, prohibitions and obligations. Automated reasoning
verifies compliance, and also derives policies for output data. This
constitutes a ``perennial'' DToU language, where the policy authoring only
occurs once, and we can conduct ongoing automated checks across users,
applications and activity cycles. Our solution is built on Turtle, Notation 3
and RDF Surfaces, for the language and the reasoning engine. It ensures
seamless integration with other semantic tools for enhanced interoperability.
We have successfully integrated this language into the Solid framework, and
conducted performance benchmark. We believe this work demonstrates a
practicality of a perennial DToU language and the potential of a paradigm shift
to how users interact with data and applications in a decentralized Web,
offering both improved privacy and usability.
\\ ( https://arxiv.org/abs/2403.07587 ,  2171kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07769
Date: Tue, 12 Mar 2024 15:56:10 GMT   (558kb)

Title: Transforming Competition into Collaboration: The Revolutionary Role of
  Multi-Agent Systems and Language Models in Modern Organizations
Authors: Carlos Jose Xavier Cruz
Categories: cs.AI cs.CL cs.CY cs.MA
\\
  This article explores the dynamic influence of computational entities based
on multi-agent systems theory (SMA) combined with large language models (LLM),
which are characterized by their ability to simulate complex human
interactions, as a possibility to revolutionize human user interaction from the
use of specialized artificial agents to support everything from operational
organizational processes to strategic decision making based on applied
knowledge and human orchestration. Previous investigations reveal that there
are limitations, particularly in the autonomous approach of artificial agents,
especially when dealing with new challenges and pragmatic tasks such as
inducing logical reasoning and problem solving. It is also considered that
traditional techniques, such as the stimulation of chains of thoughts, require
explicit human guidance. In our approach we employ agents developed from large
language models (LLM), each with distinct prototyping that considers behavioral
elements, driven by strategies that stimulate the generation of knowledge based
on the use case proposed in the scenario (role-play) business, using a
discussion approach between agents (guided conversation). We demonstrate the
potential of developing agents useful for organizational strategies, based on
multi-agent system theories (SMA) and innovative uses based on large language
models (LLM based), offering a differentiated and adaptable experiment to
different applications, complexities, domains, and capabilities from LLM.
\\ ( https://arxiv.org/abs/2403.07769 ,  558kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.06993 (*cross-listing*)
Date: Wed, 28 Feb 2024 12:34:04 GMT   (1831kb)

Title: Automatic driving lane change safety prediction model based on LSTM
Authors: Wenjian Sun, Linying Pan, Jingyu Xu, Weixiang Wan, Yong Wang
Categories: cs.RO cs.AI cs.LG cs.SY eess.IV eess.SY
\\
  Autonomous driving technology can improve traffic safety and reduce traffic
accidents. In addition, it improves traffic flow, reduces congestion, saves
energy and increases travel efficiency. In the relatively mature automatic
driving technology, the automatic driving function is divided into several
modules: perception, decision-making, planning and control, and a reasonable
division of labor can improve the stability of the system. Therefore,
autonomous vehicles need to have the ability to predict the trajectory of
surrounding vehicles in order to make reasonable decision planning and safety
measures to improve driving safety. By using deep learning method, a
safety-sensitive deep learning model based on short term memory (LSTM) network
is proposed. This model can alleviate the shortcomings of current automatic
driving trajectory planning, and the output trajectory not only ensures high
accuracy but also improves safety. The cell state simulation algorithm
simulates the trackability of the trajectory generated by this model. The
research results show that compared with the traditional model-based method,
the trajectory prediction method based on LSTM network has obvious advantages
in predicting the trajectory in the long time domain. The intention recognition
module considering interactive information has higher prediction and accuracy,
and the algorithm results show that the trajectory is very smooth based on the
premise of safe prediction and efficient lane change. And autonomous vehicles
can efficiently and safely complete lane changes.
\\ ( https://arxiv.org/abs/2403.06993 ,  1831kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06994 (*cross-listing*)
Date: Thu, 29 Feb 2024 07:50:06 GMT   (7984kb,D)

Title: Physics Sensor Based Deep Learning Fall Detection System
Authors: Zeyuan Qu and Tiange Huang and Yuxin Ji and Yongjun Li
Categories: eess.SP cs.AI cs.LG
\\
  Fall detection based on embedded sensor is a practical and popular research
direction in recent years. In terms of a specific application: fall detection
methods based upon physics sensors such as [gyroscope and accelerator] have
been exploited using traditional hand crafted features and feed them in machine
learning models like Markov chain or just threshold based classification
methods. In this paper, we build a complete system named TSFallDetect including
data receiving device based on embedded sensor, mobile deep-learning model
deploying platform, and a simple server, which will be used to gather models
and data for future expansion. On the other hand, we exploit the sequential
deep-learning methods to address this falling motion prediction problem based
on data collected by inertial and film pressure sensors. We make a empirical
study based on existing datasets and our datasets collected from our system
separately, which shows that the deep-learning model has more potential
advantage than other traditional methods, and we proposed a new deep-learning
model based on the time series data to predict the fall, and it may be superior
to other sequential models in this particular field.
\\ ( https://arxiv.org/abs/2403.06994 ,  7984kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06999 (*cross-listing*)
Date: Mon, 4 Mar 2024 10:46:02 GMT   (1517kb)

Title: Survival modeling using deep learning, machine learning and statistical
  methods: A comparative analysis for predicting mortality after hospital
  admission
Authors: Ziwen Wang, Jin Wee Lee, Tanujit Chakraborty, Yilin Ning, Mingxuan
  Liu, Feng Xie, Marcus Eng Hock Ong, Nan Liu
Categories: cs.LG cs.AI cs.CY
\\
  Survival analysis is essential for studying time-to-event outcomes and
providing a dynamic understanding of the probability of an event occurring over
time. Various survival analysis techniques, from traditional statistical models
to state-of-the-art machine learning algorithms, support healthcare
intervention and policy decisions. However, there remains ongoing discussion
about their comparative performance. We conducted a comparative study of
several survival analysis methods, including Cox proportional hazards (CoxPH),
stepwise CoxPH, elastic net penalized Cox model, Random Survival Forests (RSF),
Gradient Boosting machine (GBM) learning, AutoScore-Survival, DeepSurv,
time-dependent Cox model based on neural network (CoxTime), and DeepHit
survival neural network. We applied the concordance index (C-index) for model
goodness-of-fit, and integral Brier scores (IBS) for calibration, and
considered the model interpretability. As a case study, we performed a
retrospective analysis of patients admitted through the emergency department of
a tertiary hospital from 2017 to 2019, predicting 90-day all-cause mortality
based on patient demographics, clinicopathological features, and historical
data. The results of the C-index indicate that deep learning achieved
comparable performance, with DeepSurv producing the best discrimination
(DeepSurv: 0.893; CoxTime: 0.892; DeepHit: 0.891). The calibration of DeepSurv
(IBS: 0.041) performed the best, followed by RSF (IBS: 0.042) and GBM (IBS:
0.0421), all using the full variables. Moreover, AutoScore-Survival, using a
minimal variable subset, is easy to interpret, and can achieve good
discrimination and calibration (C-index: 0.867; IBS: 0.044). While all models
were satisfactory, DeepSurv exhibited the best discrimination and calibration.
In addition, AutoScore-Survival offers a more parsimonious model and excellent
interpretability.
\\ ( https://arxiv.org/abs/2403.06999 ,  1517kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07008 (*cross-listing*)
Date: Sat, 9 Mar 2024 02:47:11 GMT   (1094kb,D)

Title: AutoEval Done Right: Using Synthetic Data for Model Evaluation
Authors: Pierre Boyeau, Anastasios N. Angelopoulos, Nir Yosef, Jitendra Malik,
  Michael I. Jordan
Categories: cs.LG cs.AI cs.CL stat.ME
\\
  The evaluation of machine learning models using human-labeled validation data
can be expensive and time-consuming. AI-labeled synthetic data can be used to
decrease the number of human annotations required for this purpose in a process
called autoevaluation. We suggest efficient and statistically principled
algorithms for this purpose that improve sample efficiency while remaining
unbiased. These algorithms increase the effective human-labeled sample size by
up to 50% on experiments with GPT-4.
\\ ( https://arxiv.org/abs/2403.07008 ,  1094kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07017 (*cross-listing*)
Date: Sat, 9 Mar 2024 17:36:54 GMT   (694kb,D)

Title: Mathematics of multi-agent learning systems at the interface of game
  theory and artificial intelligence
Authors: Long Wang, Feng Fu, Xingru Chen
Categories: physics.soc-ph cs.AI cs.GT cs.MA
Comments: 8 pages, 1 figure
\\
  Evolutionary Game Theory (EGT) and Artificial Intelligence (AI) are two
fields that, at first glance, might seem distinct, but they have notable
connections and intersections. The former focuses on the evolution of behaviors
(or strategies) in a population, where individuals interact with others and
update their strategies based on imitation (or social learning). The more
successful a strategy is, the more prevalent it becomes over time. The latter,
meanwhile, is centered on machine learning algorithms and (deep) neural
networks. It is often from a single-agent perspective but increasingly involves
multi-agent environments, in which intelligent agents adjust their strategies
based on feedback and experience, somewhat akin to the evolutionary process yet
distinct in their self-learning capacities. In light of the key components
necessary to address real-world problems, including (i) learning and
adaptation, (ii) cooperation and competition, (iii) robustness and stability,
and altogether (iv) population dynamics of individual agents whose strategies
evolve, the cross-fertilization of ideas between both fields will contribute to
the advancement of mathematics of multi-agent learning systems, in particular,
to the nascent domain of ``collective cooperative intelligence'' bridging
evolutionary dynamics and multi-agent reinforcement learning.
\\ ( https://arxiv.org/abs/2403.07017 ,  694kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07022 (*cross-listing*)
Date: Sun, 10 Mar 2024 02:34:44 GMT   (11707kb,D)

Title: A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary
  Modifiable Areal Units
Authors: Liyue Chen, Jiangyi Fang, Tengfei Liu, Shaosheng Cao, Leye Wang
Categories: cs.LG cs.AI
Comments: Accepted by ICDE 2024
\\
  Spatio-Temporal (ST) prediction is crucial for making informed decisions in
urban location-based applications like ride-sharing. However, existing ST
models often require region partition as a prerequisite, resulting in two main
pitfalls. Firstly, location-based services necessitate ad-hoc regions for
various purposes, requiring multiple ST models with varying scales and zones,
which can be costly to support. Secondly, different ST models may produce
conflicting outputs, resulting in confusing predictions. In this paper, we
propose One4All-ST, a framework that can conduct ST prediction for arbitrary
modifiable areal units using only one model. To reduce the cost of getting
multi-scale predictions, we design an ST network with hierarchical spatial
modeling and scale normalization modules to efficiently and equally learn
multi-scale representations. To address prediction inconsistencies across
scales, we propose a dynamic programming scheme to solve the formulated optimal
combination problem, minimizing predicted error through theoretical analysis.
Besides, we suggest using an extended quad-tree to index the optimal
combinations for quick response to arbitrary modifiable areal units in
practical online scenarios. Extensive experiments on two real-world datasets
verify the efficiency and effectiveness of One4All-ST in ST prediction for
arbitrary modifiable areal units. The source codes and data of this work are
available at https://github.com/uctb/One4All-ST.
\\ ( https://arxiv.org/abs/2403.07022 ,  11707kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07028 (*cross-listing*)
Date: Mon, 11 Mar 2024 02:17:42 GMT   (7759kb,D)

Title: An Efficient Learning-based Solver Comparable to Metaheuristics for the
  Capacitated Arc Routing Problem
Authors: Runze Guo, Feng Xue, Anlong Ming, Nicu Sebe
Categories: cs.LG cs.AI math.OC
\\
  Recently, neural networks (NN) have made great strides in combinatorial
optimization. However, they face challenges when solving the capacitated arc
routing problem (CARP) which is to find the minimum-cost tour covering all
required edges on a graph, while within capacity constraints. In tackling CARP,
NN-based approaches tend to lag behind advanced metaheuristics, since they lack
directed arc modeling and efficient learning methods tailored for complex CARP.
In this paper, we introduce an NN-based solver to significantly narrow the gap
with advanced metaheuristics while exhibiting superior efficiency. First, we
propose the direction-aware attention model (DaAM) to incorporate
directionality into the embedding process, facilitating more effective
one-stage decision-making. Second, we design a supervised reinforcement
learning scheme that involves supervised pre-training to establish a robust
initial policy for subsequent reinforcement fine-tuning. It proves particularly
valuable for solving CARP that has a higher complexity than the node routing
problems (NRPs). Finally, a path optimization method is proposed to adjust the
depot return positions within the path generated by DaAM. Experiments
illustrate that our approach surpasses heuristics and achieves decision quality
comparable to state-of-the-art metaheuristics for the first time while
maintaining superior efficiency.
\\ ( https://arxiv.org/abs/2403.07028 ,  7759kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07032 (*cross-listing*)
Date: Mon, 11 Mar 2024 04:56:10 GMT   (2608kb,D)

Title: STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning
  for Real-world Scene Flow
Authors: Zhiyang Lu and Qinghan Chen and Ming Cheng
Categories: cs.CV cs.AI
Comments: 10 pages, 8 figures, CVPR template
\\
  Scene flow prediction is a crucial underlying task in understanding dynamic
scenes as it offers fundamental motion information. However, contemporary scene
flow methods encounter three major challenges. Firstly, flow estimation solely
based on local receptive fields lacks long-dependency matching of point pairs.
To address this issue, we propose global attentive flow embedding to match
all-to-all point pairs in both feature space and Euclidean space, providing
global initialization before local refinement. Secondly, there are deformations
existing in non-rigid objects after warping, which leads to variations in the
spatiotemporal relation between the consecutive frames. For a more precise
estimation of residual flow, a spatial temporal feature re-embedding module is
devised to acquire the sequence features after deformation. Furthermore,
previous methods perform poor generalization due to the significant domain gap
between the synthesized and LiDAR-scanned datasets. We leverage novel domain
adaptive losses to effectively bridge the gap of motion inference from
synthetic to real-world. Experiments demonstrate that our approach achieves
state-of-the-art performance across various datasets, with particularly
outstanding results on real-world LiDAR-scanned datasets. Our code is available
at https://github.com/O-VIGIA/StarFlow.
\\ ( https://arxiv.org/abs/2403.07032 ,  2608kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07033 (*cross-listing*)
Date: Mon, 11 Mar 2024 05:47:07 GMT   (3424kb,D)

Title: Interpreting What Typical Fault Signals Look Like via Prototype-matching
Authors: Qian Chen and Xingjian Dong and Zhike Peng
Categories: cs.LG cs.AI
Comments: 17 pages, 12 figures, 6 tables
\\
  Neural networks, with powerful nonlinear mapping and classification
capabilities, are widely applied in mechanical fault diagnosis to ensure
safety. However, being typical black-box models, their application is limited
in high-reliability-required scenarios. To understand the classification logic
and explain what typical fault signals look like, the prototype matching
network (PMN) is proposed by combining the human-inherent prototype-matching
with autoencoder (AE). The PMN matches AE-extracted feature with each prototype
and selects the most similar prototype as the prediction result. It has three
interpreting paths on classification logic, fault prototypes, and matching
contributions. Conventional diagnosis and domain generalization experiments
demonstrate its competitive diagnostic performance and distinguished advantages
in representation learning. Besides, the learned typical fault signals (i.e.,
sample-level prototypes) showcase the ability for denoising and extracting
subtle key features that experts find challenging to capture. This ability
broadens human understanding and provides a promising solution from
interpretability research to AI-for-Science.
\\ ( https://arxiv.org/abs/2403.07033 ,  3424kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07039 (*cross-listing*)
Date: Mon, 11 Mar 2024 09:57:16 GMT   (669kb)

Title: From English to ASIC: Hardware Implementation with Large Language Model
Authors: Emil Goh, Maoyang Xiang, I-Chyn Wey, T. Hui Teo
Categories: cs.AR cs.AI cs.PL
Comments: 15 pages, 1 figure
\\
  In the realm of ASIC engineering, the landscape has been significantly
reshaped by the rapid development of LLM, paralleled by an increase in the
complexity of modern digital circuits. This complexity has escalated the
requirements for HDL coding, necessitating a higher degree of precision and
sophistication. However, challenges have been faced due to the
less-than-optimal performance of modern language models in generating hardware
description code, a situation further exacerbated by the scarcity of the
corresponding high-quality code datasets. These challenges have highlighted the
gap between the potential of LLMs to revolutionize digital circuit design and
their current capabilities in accurately interpreting and implementing hardware
specifications. To address these challenges, a strategy focusing on the
fine-tuning of the leading-edge nature language model and the reshuffling of
the HDL code dataset has been developed. The fine-tuning aims to enhance
models' proficiency in generating precise and efficient ASIC design, while the
dataset reshuffling is intended to broaden the scope and improve the quality of
training material. The model demonstrated significant improvements compared to
the base model, with approximately 10% to 20% increase in accuracy across a
wide range of temperature for the pass@1 metric. This approach is expected to
facilitate a simplified and more efficient LLM-assisted framework for complex
circuit design, leveraging their capabilities to meet the sophisticated demands
of HDL coding and thus streamlining the ASIC development process.
\\ ( https://arxiv.org/abs/2403.07039 ,  669kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07040 (*cross-listing*)
Date: Mon, 11 Mar 2024 16:04:58 GMT   (1072kb,D)

Title: All in One: Multi-Task Prompting for Graph Neural Networks (Extended
  Abstract)
Authors: Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan
Categories: cs.LG cs.AI
Comments: submitted to IJCAI 2024 Sister Conferences Track. The original paper
  can be seen at arXiv:2307.01504
\\
  This paper is an extended abstract of our original work published in KDD23,
where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li,
Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural
networks. KDD 23) The paper introduces a novel approach to bridging the gap
between pre-trained graph models and the diverse tasks they're applied to,
inspired by the success of prompt learning in NLP. Recognizing the challenge of
aligning pre-trained models with varied graph tasks (node level, edge level,
and graph level), which can lead to negative transfer and poor performance, we
propose a multi-task prompting method for graphs. This method involves unifying
graph and language prompt formats, enabling NLP's prompting strategies to be
adapted for graph tasks. By analyzing the task space of graph applications, we
reformulate problems to fit graph-level tasks and apply meta-learning to
improve prompt initialization for multiple tasks. Experiments show our method's
effectiveness in enhancing model performance across different graph tasks.
  Beyond the original work, in this extended abstract, we further discuss the
graph prompt from a bigger picture and provide some of the latest work toward
this area.
\\ ( https://arxiv.org/abs/2403.07040 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07078 (*cross-listing*)
Date: Mon, 11 Mar 2024 18:11:00 GMT   (2816kb)

Title: Improving deep learning with prior knowledge and cognitive models: A
  survey on enhancing explainability, adversarial robustness and zero-shot
  learning
Authors: Fuseinin Mumuni and Alhassan Mumuni
Categories: cs.LG cs.AI cs.CV
Journal-ref: Cognitive Systems Research, 84 (2024)
DOI: 10.1016/j.cogsys.2023.101188
\\
  We review current and emerging knowledge-informed and brain-inspired
cognitive systems for realizing adversarial defenses, eXplainable Artificial
Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep
learning models have achieved remarkable performance and demonstrated
capabilities surpassing human experts in many applications. Yet, their
inability to exploit domain knowledge leads to serious performance limitations
in practical applications. In particular, deep learning systems are exposed to
adversarial attacks, which can trick them into making glaringly incorrect
decisions. Moreover, complex data-driven models typically lack interpretability
or explainability, i.e., their decisions cannot be understood by human
subjects. Furthermore, models are usually trained on standard datasets with a
closed-world assumption. Hence, they struggle to generalize to unseen cases
during inference in practical open-world environments, thus, raising the zero-
or few-shot generalization problem. Although many conventional solutions exist,
explicit domain knowledge, brain-inspired neural network and cognitive
architectures offer powerful new dimensions towards alleviating these problems.
Prior knowledge is represented in appropriate forms and incorporated in deep
learning frameworks to improve performance. Brain-inspired cognition methods
use computational models that mimic the human mind to enhance intelligent
behavior in artificial agents and autonomous robots. Ultimately, these models
achieve better explainability, higher adversarial robustness and data-efficient
learning, and can, in turn, provide insights for cognitive science and
neuroscience-that is, to deepen human understanding on how the brain works in
general, and how it handles these problems.
\\ ( https://arxiv.org/abs/2403.07078 ,  2816kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07087 (*cross-listing*)
Date: Mon, 11 Mar 2024 18:25:01 GMT   (814kb)

Title: LSTM-Based Text Generation: A Study on Historical Datasets
Authors: Mustafa Abbas Hussein Hussein, Serkan Sava\c{s}
Categories: cs.CL cs.AI
Report-no: ISBN: 978-625-6879-50-8
Journal-ref: 16th International Istanbul Scientific Research Congress on Life,
  Engineering, Architecture, and Mathematical Sciences Proceedings Book, Pages:
  42-49, 2024
DOI: 10.5281/zenodo.10776102
\\
  This paper presents an exploration of Long Short-Term Memory (LSTM) networks
in the realm of text generation, focusing on the utilization of historical
datasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in
handling sequential data, are applied here to model complex language patterns
and structures inherent in historical texts. The study demonstrates that
LSTM-based models, when trained on historical datasets, can not only generate
text that is linguistically rich and contextually relevant but also provide
insights into the evolution of language patterns over time. The finding
presents models that are highly accurate and efficient in predicting text from
works of Nietzsche, with low loss values and a training time of 100 iterations.
The accuracy of the model is 0.9521, indicating high accuracy. The loss of the
model is 0.2518, indicating its effectiveness. The accuracy of the model in
predicting text from the work of Shakespeare is 0.9125, indicating a low error
rate. The training time of the model is 100, mirroring the efficiency of the
Nietzsche dataset. This efficiency demonstrates the effectiveness of the model
design and training methodology, especially when handling complex literary
texts. This research contributes to the field of natural language processing by
showcasing the versatility of LSTM networks in text generation and offering a
pathway for future explorations in historical linguistics and beyond.
\\ ( https://arxiv.org/abs/2403.07087 ,  814kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07090 (*cross-listing*)
Date: Mon, 11 Mar 2024 18:33:56 GMT   (565kb,D)

Title: Time Series Analysis of Key Societal Events as Reflected in Complex
  Social Media Data Streams
Authors: Andy Skumanich, Han Kyul Kim
Categories: cs.IR cs.AI
Comments: AAAI2024 Workshop on AI for Time Series Analysis (AI4TS)
\\
  Social media platforms hold valuable insights, yet extracting essential
information can be challenging. Traditional top-down approaches often struggle
to capture critical signals in rapidly changing events. As global events evolve
swiftly, social media narratives, including instances of disinformation, become
significant sources of insights. To address the need for an inductive strategy,
we explore a niche social media platform GAB and an established messaging
service Telegram, to develop methodologies applicable on a broader scale. This
study investigates narrative evolution on these platforms using quantitative
corpus-based discourse analysis techniques. Our approach is a novel mode to
study multiple social media domains to distil key information which may be
obscured otherwise, allowing for useful and actionable insights. The paper
details the technical and methodological aspects of gathering and preprocessing
GAB and Telegram data for a keyness (Log Ratio) metric analysis, identifying
crucial nouns and verbs for deeper exploration. Empirically, this approach is
applied to a case study of a well defined event that had global impact: the
2023 Wagner mutiny. The main findings are: (1) the time line can be
deconstructed to provide useful data features allowing for improved
interpretation; (2) a methodology is applied which provides a basis for
generalization. The key contribution is an approach, that in some cases,
provides the ability to capture the dynamic narrative shifts over time with
elevated confidence. The approach can augment near-real-time assessment of key
social movements, allowing for informed governance choices. This research is
important because it lays out a useful methodology for time series relevant
info-culling, which can enable proactive modes for positive social engagement.
\\ ( https://arxiv.org/abs/2403.07090 ,  565kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07136 (*cross-listing*)
Date: Mon, 11 Mar 2024 20:05:48 GMT   (37kb)

Title: On the Limited Representational Power of Value Functions and its Links
  to Statistical (In)Efficiency
Authors: David Cheikhi, Daniel Russo
Categories: cs.LG cs.AI stat.ML
\\
  Identifying the trade-offs between model-based and model-free methods is a
central question in reinforcement learning. Value-based methods offer
substantial computational advantages and are sometimes just as statistically
efficient as model-based methods. However, focusing on the core problem of
policy evaluation, we show information about the transition dynamics may be
impossible to represent in the space of value functions. We explore this
through a series of case studies focused on structures that arises in many
important problems. In several, there is no information loss and value-based
methods are as statistically efficient as model based ones. In other
closely-related examples, information loss is severe and value-based methods
are severely outperformed. A deeper investigation points to the limitations of
the representational power as the driver of the inefficiency, as opposed to
failure in algorithm design.
\\ ( https://arxiv.org/abs/2403.07136 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07151 (*cross-listing*)
Date: Mon, 11 Mar 2024 20:39:32 GMT   (612kb,D)

Title: Don't Forget What I did?: Assessing Client Contributions in Federated
  Learning
Authors: Bishwamittra Ghosh, Debabrota Basu, Fu Huazhu, Wang Yuan, Renuga
  Kanagavelu, Jiang Jin Peng, Liu Yong, Goh Siow Mong Rick, and Wei Qingsong
Categories: cs.LG cs.AI cs.CR
Comments: Under submission
\\
  Federated Learning (FL) is a collaborative machine learning (ML) approach,
where multiple clients participate in training an ML model without exposing the
private data. Fair and accurate assessment of client contributions is an
important problem in FL to facilitate incentive allocation and encouraging
diverse clients to participate in a unified model training. Existing methods
for assessing client contribution adopts co-operative game-theoretic concepts,
such as Shapley values, but under simplified assumptions. In this paper, we
propose a history-aware game-theoretic framework, called FLContrib, to assess
client contributions when a subset of (potentially non-i.i.d.) clients
participate in each epoch of FL training. By exploiting the FL training process
and linearity of Shapley value, we develop FLContrib that yields a historical
timeline of client contributions as FL training progresses over epochs.
Additionally, to assess client contribution under limited computational budget,
we propose a scheduling procedure that considers a two-sided fairness criteria
to perform expensive Shapley value computation only in a subset of training
epochs. In experiments, we demonstrate a controlled trade-off between the
correctness and efficiency of client contributions assessed via FLContrib. To
demonstrate the benefits of history-aware client contributions, we apply
FLContrib to detect dishonest clients conducting data poisoning in FL training.
\\ ( https://arxiv.org/abs/2403.07151 ,  612kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07175 (*cross-listing*)
Date: Mon, 11 Mar 2024 21:33:05 GMT   (571kb,D)

Title: Rebuilding ROME : Resolving Model Collapse during Sequential Model
  Editing
Authors: Akshat Gupta, Gopala Anumanchipalli
Categories: cs.CL cs.AI
\\
  Recent work on model editing using Rank-One Model Editing (ROME), a popular
model editing method, has shown that there are certain facts that the algorithm
is unable to edit without breaking the model. Such edits have previously been
called disabling edits. These disabling edits cause immediate model collapse
and limits the use of ROME for sequential editing. In this paper, we make two
main contributions. Firstly, we show that model collapse with ROME only happens
when making edits using the CounterFact dataset and does not happen when using
the zsRE dataset. Secondly, we find that disabling edits are an artifact of the
original implementation of ROME. With this paper, we provide a more stable
implementation ROME, which we call r-ROME and show that we no longer observe
model collapse when making large scale sequential edits with ROME.
\\ ( https://arxiv.org/abs/2403.07175 ,  571kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07183 (*cross-listing*)
Date: Mon, 11 Mar 2024 21:51:39 GMT   (570kb,D)

Title: Monitoring AI-Modified Content at Scale: A Case Study on the Impact of
  ChatGPT on AI Conference Peer Reviews
Authors: Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao,
  Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, Daniel A.
  McFarland, James Y. Zou
Categories: cs.CL cs.AI cs.LG cs.SI
Comments: 42 pages, 30 figures
ACM-class: I.2.7
\\
  We present an approach for estimating the fraction of text in a large corpus
which is likely to be substantially modified or produced by a large language
model (LLM). Our maximum likelihood model leverages expert-written and
AI-generated reference texts to accurately and efficiently examine real-world
LLM-use at the corpus level. We apply this approach to a case study of
scientific peer review in AI conferences that took place after the release of
ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest
that between 6.5% and 16.9% of text submitted as peer reviews to these
conferences could have been substantially modified by LLMs, i.e. beyond
spell-checking or minor writing updates. The circumstances in which generated
text occurs offer insight into user behavior: the estimated fraction of
LLM-generated text is higher in reviews which report lower confidence, were
submitted close to the deadline, and from reviewers who are less likely to
respond to author rebuttals. We also observe corpus-level trends in generated
text which may be too subtle to detect at the individual level, and discuss the
implications of such trends on peer review. We call for future
interdisciplinary work to examine how LLM use is changing our information and
knowledge practices.
\\ ( https://arxiv.org/abs/2403.07183 ,  570kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07191 (*cross-listing*)
Date: Mon, 11 Mar 2024 22:24:14 GMT   (8928kb,D)

Title: $\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking
  Reinforcement Learning Algorithms in Generative Language Model
Authors: Yufeng Zhang, Liyu Chen, Boyi Liu, Yingxiang Yang, Qiwen Cui, Yunzhe
  Tao, Hongxia Yang
Categories: cs.LG cs.AI cs.CL
Comments: 8 pages
\\
  Recent advances in reinforcement learning (RL) algorithms aim to enhance the
performance of language models at scale. Yet, there is a noticeable absence of
a cost-effective and standardized testbed tailored to evaluating and comparing
these algorithms. To bridge this gap, we present a generalized version of the
24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a
target value $K$ with $N$ integers. We evaluate the effectiveness of
established RL algorithms such as Proximal Policy Optimization (PPO), alongside
novel approaches like Identity Policy Optimization (IPO) and Direct Policy
Optimization (DPO).
\\ ( https://arxiv.org/abs/2403.07191 ,  8928kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07193 (*cross-listing*)
Date: Mon, 11 Mar 2024 22:27:16 GMT   (873kb)

Title: CuentosIE: can a chatbot about "tales with a message" help to teach
  emotional intelligence?
Authors: Antonio Ferr\'andez, Roc\'io Lavigne-Cerv\'an, Jes\'us Peral, Ignasi
  Navarro-Soria, \'Angel Lloret, David Gil, Carmen Rocamora
Categories: cs.CL cs.AI
Comments: 26 pages
ACM-class: I.2.0
Journal-ref: PeerJ Computer Science, Volume 10, February 2024, ID e1866
DOI: 10.7717/peerj-cs.1866
\\
  In this article, we present CuentosIE (TalesEI: chatbot of tales with a
message to develop Emotional Intelligence), an educational chatbot on emotions
that also provides teachers and psychologists with a tool to monitor their
students/patients through indicators and data compiled by CuentosIE. The use of
"tales with a message" is justified by their simplicity and easy understanding,
thanks to their moral or associated metaphors. The main contributions of
CuentosIE are the selection, collection, and classification of a set of highly
specialized tales, as well as the provision of tools (searching, reading
comprehension, chatting, recommending, and classifying) that are useful for
both educating users about emotions and monitoring their emotional development.
The preliminary evaluation of the tool has obtained encouraging results, which
provides an affirmative answer to the question posed in the title of the
article.
\\ ( https://arxiv.org/abs/2403.07193 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07194 (*cross-listing*)
Date: Sat, 10 Feb 2024 09:31:39 GMT   (536kb)

Title: Improving prediction of students' performance in intelligent tutoring
  systems using attribute selection and ensembles of different multimodal data
  sources
Authors: W. Chango, R. Cerezo, M. Sanchez-Santillan, R. Azevedo, and C. Romero
Categories: cs.CY cs.AI cs.HC cs.LG
Journal-ref: Journal of Computing in Higher Education,2021, 33, 614-634
DOI: 10.1007/s12528-021-09298-8
\\
  The aim of this study was to predict university students' learning
performance using different sources of data from an Intelligent Tutoring
System. We collected and preprocessed data from 40 students from different
multimodal sources: learning strategies from system logs, emotions from face
recording videos, interaction zones from eye tracking, and test performance
from final knowledge evaluation. Our objective was to test whether the
prediction could be improved by using attribute selection and classification
ensembles. We carried out three experiments by applying six classification
algorithms to numerical and discretized preprocessed multimodal data. The
results show that the best predictions were produced using ensembles and
selecting the best attributes approach with numerical data.
\\ ( https://arxiv.org/abs/2403.07194 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07201 (*cross-listing*)
Date: Mon, 11 Mar 2024 22:58:11 GMT   (1143kb)

Title: A multi-cohort study on prediction of acute brain dysfunction states
  using selective state space models
Authors: Brandon Silva, Miguel Contreras, Sabyasachi Bandyopadhyay, Yuanfang
  Ren, Ziyuan Guan, Jeremy Balch, Kia Khezeli, Tezcan Ozrazgat Baslanti, Ben
  Shickel, Azra Bihorac, Parisa Rashidi
Categories: cs.LG cs.AI stat.AP
Comments: 22 pages, 8 figures, To be published
\\
  Assessing acute brain dysfunction (ABD), including delirium and coma in the
intensive care unit (ICU), is a critical challenge due to its prevalence and
severe implications for patient outcomes. Current diagnostic methods rely on
infrequent clinical observations, which can only determine a patient's ABD
status after onset. Our research attempts to solve these problems by harnessing
Electronic Health Records (EHR) data to develop automated methods for ABD
prediction for patients in the ICU. Existing models solely predict a single
state (e.g., either delirium or coma), require at least 24 hours of observation
data to make predictions, do not dynamically predict fluctuating ABD conditions
during ICU stay (typically a one-time prediction), and use small sample size,
proprietary single-hospital datasets. Our research fills these gaps in the
existing literature by dynamically predicting delirium, coma, and mortality for
12-hour intervals throughout an ICU stay and validating on two public datasets.
Our research also introduces the concept of dynamically predicting critical
transitions from non-ABD to ABD and between different ABD states in real time,
which could be clinically more informative for the hospital staff. We compared
the predictive performance of two state-of-the-art neural network models, the
MAMBA selective state space model and the Longformer Transformer model. Using
the MAMBA model, we achieved a mean area under the receiving operator
characteristic curve (AUROC) of 0.95 on outcome prediction of ABD for 12-hour
intervals. The model achieves a mean AUROC of 0.79 when predicting transitions
between ABD states. Our study uses a curated dataset from the University of
Florida Health Shands Hospital for internal validation and two publicly
available datasets, MIMIC-IV and eICU, for external validation, demonstrating
robustness across ICU stays from 203 hospitals and 140,945 patients.
\\ ( https://arxiv.org/abs/2403.07201 ,  1143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07230 (*cross-listing*)
Date: Tue, 12 Mar 2024 00:58:19 GMT   (821kb,D)

Title: Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked
  Preferences
Authors: Pulkit Pattnaik and Rishabh Maheshwary and Kelechi Ogueji and Vikas
  Yadav and Sathwik Tejaswi Madhusudhan
Categories: cs.CL cs.AI cs.LG
Comments: Work in progress
\\
  Direct Preference Optimization (DPO) is an effective technique that leverages
pairwise preference data (usually one chosen and rejected response pair per
user prompt) to align LLMs to human preferences. In practice, multiple
responses can exist for a given prompt with varying quality relative to each
other. With availability of such quality ratings for multiple responses, we
propose utilizing these responses to create multiple preference pairs for a
given prompt. Our work focuses on systematically using the constructed multiple
preference pair in DPO training via curriculum learning methodology. In
particular, we order these multiple pairs of preference data from easy to hard
(emulating curriculum training) according to various criteria. We show detailed
comparisons of our proposed approach to the standard single-pair DPO setting.
Our method, which we call Curry-DPO consistently shows increased performance
gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,
highlighting its effectiveness. More specifically, Curry-DPO achieves a score
of 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs
with similar parameter size. Curry-DPO also achieves the highest adjusted win
rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and
87.9% respectively) in our experiments, with notable gains of upto 7.5% when
compared to standard DPO technique.
\\ ( https://arxiv.org/abs/2403.07230 ,  821kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07255 (*cross-listing*)
Date: Tue, 12 Mar 2024 02:24:37 GMT   (2327kb,D)

Title: Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free
  NOMA in Machine-Type Communication
Authors: Yongjeong Oh, Jaehong Jo, Byonghyo Shim, and Yo-Seb Jeon
Categories: eess.SP cs.AI cs.LG
\\
  In this paper, we present a novel approach for joint activity detection (AD),
channel estimation (CE), and data detection (DD) in uplink grant-free
non-orthogonal multiple access (NOMA) systems. Our approach employs an
iterative and parallel interference removal strategy inspired by parallel
interference cancellation (PIC), enhanced with deep learning to jointly tackle
the AD, CE, and DD problems. Based on this approach, we develop three PIC
frameworks, each of which is designed for either coherent or non-coherence
schemes. The first framework performs joint AD and CE using received pilot
signals in the coherent scheme. Building upon this framework, the second
framework utilizes both the received pilot and data signals for CE, further
enhancing the performances of AD, CE, and DD in the coherent scheme. The third
framework is designed to accommodate the non-coherent scheme involving a small
number of data bits, which simultaneously performs AD and DD. Through joint
loss functions and interference cancellation modules, our approach supports
end-to-end training, contributing to enhanced performances of AD, CE, and DD
for both coherent and non-coherent schemes. Simulation results demonstrate the
superiority of our approach over traditional techniques, exhibiting enhanced
performances of AD, CE, and DD while maintaining lower computational
complexity.
\\ ( https://arxiv.org/abs/2403.07255 ,  2327kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07261 (*cross-listing*)
Date: Tue, 12 Mar 2024 02:38:36 GMT   (3468kb,D)

Title: Disentangling Policy from Offline Task Representation Learning via
  Adversarial Data Augmentation
Authors: Chengxing Jia, Fuxiang Zhang, Yi-Chen Li, Chen-Xiao Gao, Xu-Hui Liu,
  Lei Yuan, Zongzhang Zhang, Yang Yu
Categories: cs.LG cs.AI
\\
  Offline meta-reinforcement learning (OMRL) proficiently allows an agent to
tackle novel tasks while solely relying on a static dataset. For precise and
efficient task identification, existing OMRL research suggests learning
separate task representations that be incorporated with policy input, thus
forming a context-based meta-policy. A major approach to train task
representations is to adopt contrastive learning using multi-task offline data.
The dataset typically encompasses interactions from various policies (i.e., the
behavior policies), thus providing a plethora of contextual information
regarding different tasks. Nonetheless, amassing data from a substantial number
of policies is not only impractical but also often unattainable in realistic
settings. Instead, we resort to a more constrained yet practical scenario,
where multi-task data collection occurs with a limited number of policies. We
observed that learned task representations from previous OMRL methods tend to
correlate spuriously with the behavior policy instead of reflecting the
essential characteristics of the task, resulting in unfavorable
out-of-distribution generalization. To alleviate this issue, we introduce a
novel algorithm to disentangle the impact of behavior policy from task
representation learning through a process called adversarial data augmentation.
Specifically, the objective of adversarial data augmentation is not merely to
generate data analogous to offline data distribution; instead, it aims to
create adversarial examples designed to confound learned task representations
and lead to incorrect task identification. Our experiments show that learning
from such adversarial samples significantly enhances the robustness and
effectiveness of the task identification process and realizes satisfactory
out-of-distribution generalization.
\\ ( https://arxiv.org/abs/2403.07261 ,  3468kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07262 (*cross-listing*)
Date: Tue, 12 Mar 2024 02:43:41 GMT   (2785kb,D)

Title: Advantage-Aware Policy Optimization for Offline Reinforcement Learning
Authors: Yunpeng Qing, Shunyu liu, Jingyuan Cong, Kaixuan Chen, Yihe Zhou,
  Mingli Song
Categories: cs.LG cs.AI
\\
  Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to
craft effective agent policy without online interaction, which imposes proper
conservative constraints with the support of behavior policies to tackle the
Out-Of-Distribution (OOD) problem. However, existing works often suffer from
the constraint conflict issue when offline datasets are collected from multiple
behavior policies, i.e., different behavior policies may exhibit inconsistent
actions with distinct returns across the state space. To remedy this issue,
recent Advantage-Weighted (AW) methods prioritize samples with high advantage
values for agent training while inevitably leading to overfitting on these
samples. In this paper, we introduce a novel Advantage-Aware Policy
Optimization (A2PO) method to explicitly construct advantage-aware policy
constraints for offline learning under mixed-quality datasets. Specifically,
A2PO employs a Conditional Variational Auto-Encoder (CVAE) to disentangle the
action distributions of intertwined behavior policies by modeling the advantage
values of all training data as conditional variables. Then the agent can follow
such disentangled action distribution constraints to optimize the
advantage-aware policy towards high advantage values. Extensive experiments
conducted on both the single-quality and mixed-quality datasets of the D4RL
benchmark demonstrate that A2PO yields results superior to state-of-the-art
counterparts. Our code will be made publicly available.
\\ ( https://arxiv.org/abs/2403.07262 ,  2785kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07271 (*cross-listing*)
Date: Tue, 12 Mar 2024 03:00:15 GMT   (3020kb,D)

Title: Anderson acceleration for iteratively reweighted $\ell_1$ algorithm
Authors: Kexin Li
Categories: math.OC cs.AI cs.LG eess.SP
\\
  Iteratively reweighted L1 (IRL1) algorithm is a common algorithm for solving
sparse optimization problems with nonconvex and nonsmooth regularization. The
development of its acceleration algorithm, often employing Nesterov
acceleration, has sparked significant interest. Nevertheless, the convergence
and complexity analysis of these acceleration algorithms consistently poses
substantial challenges. Recently, Anderson acceleration has gained prominence
owing to its exceptional performance for speeding up fixed-point iteration,
with numerous recent studies applying it to gradient-based algorithms.
Motivated by the powerful impact of Anderson acceleration, we propose an
Anderson-accelerated IRL1 algorithm and establish its local linear convergence
rate. We extend this convergence result, typically observed in smooth settings,
to a nonsmooth scenario. Importantly, our theoretical results do not depend on
the Kurdyka-Lojasiewicz condition, a necessary condition in existing Nesterov
acceleration-based algorithms. Furthermore, to ensure global convergence, we
introduce a globally convergent Anderson accelerated IRL1 algorithm by
incorporating a classical nonmonotone line search condition. Experimental
results indicate that our algorithm outperforms existing Nesterov
acceleration-based algorithms.
\\ ( https://arxiv.org/abs/2403.07271 ,  3020kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07277 (*cross-listing*)
Date: Tue, 12 Mar 2024 03:15:08 GMT   (39023kb,D)

Title: A Bayesian Approach to OOD Robustness in Image Classification
Authors: Prakhar Kaushik and Adam Kortylewski and Alan Yuille
Categories: cs.CV cs.AI
Comments: CVPR 2024
\\
  An important and unsolved problem in computer vision is to ensure that the
algorithms are robust to changes in image domains. We address this problem in
the scenario where we have access to images from the target domains but no
annotations. Motivated by the challenges of the OOD-CV benchmark where we
encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce
a novel Bayesian approach to OOD robustness for object classification. Our work
extends Compositional Neural Networks (CompNets), which have been shown to be
robust to occlusion but degrade badly when tested on OOD data. We exploit the
fact that CompNets contain a generative head defined over feature vectors
represented by von Mises-Fisher (vMF) kernels, which correspond roughly to
object parts, and can be learned without supervision. We obverse that some vMF
kernels are similar between different domains, while others are not. This
enables us to learn a transitional dictionary of vMF kernels that are
intermediate between the source and target domains and train the generative
model on this dictionary using the annotations on the source domain, followed
by iterative refinement. This approach, termed Unsupervised Generative
Transition (UGT), performs very well in OOD scenarios even when occlusion is
present. UGT is evaluated on different OOD benchmarks including the OOD-CV
dataset, several popular datasets (e.g., ImageNet-C [9]), artificial image
corruptions (including adding occluders), and synthetic-to-real domain
transfer, and does well in all scenarios outperforming SOTA alternatives (e.g.
up to 10% top-1 accuracy on Occluded OOD-CV dataset).
\\ ( https://arxiv.org/abs/2403.07277 ,  39023kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07292 (*cross-listing*)
Date: Tue, 12 Mar 2024 03:50:57 GMT   (27114kb,D)

Title: Continual All-in-One Adverse Weather Removal with Knowledge Replay on a
  Unified Network Structure
Authors: De Cheng, Yanling Ji, Dong Gong, Yan Li, Nannan Wang, Junwei Han,
  Dingwen Zhang
Categories: cs.CV cs.AI
\\
  In real-world applications, image degeneration caused by adverse weather is
always complex and changes with different weather conditions from days and
seasons. Systems in real-world environments constantly encounter adverse
weather conditions that are not previously observed. Therefore, it practically
requires adverse weather removal models to continually learn from incrementally
collected data reflecting various degeneration types. Existing adverse weather
removal approaches, for either single or multiple adverse weathers, are mainly
designed for a static learning paradigm, which assumes that the data of all
types of degenerations to handle can be finely collected at one time before a
single-phase learning process. They thus cannot directly handle the incremental
learning requirements. To address this issue, we made the earliest effort to
investigate the continual all-in-one adverse weather removal task, in a setting
closer to real-world applications. Specifically, we develop a novel continual
learning framework with effective knowledge replay (KR) on a unified network
structure. Equipped with a principal component projection and an effective
knowledge distillation mechanism, the proposed KR techniques are tailored for
the all-in-one weather removal task. It considers the characteristics of the
image restoration task with multiple degenerations in continual learning, and
the knowledge for different degenerations can be shared and accumulated in the
unified network structure. Extensive experimental results demonstrate the
effectiveness of the proposed method to deal with this challenging task, which
performs competitively to existing dedicated or joint training image
restoration methods. Our code is available at
https://github.com/xiaojihh/CL_all-in-one.
\\ ( https://arxiv.org/abs/2403.07292 ,  27114kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07294 (*cross-listing*)
Date: Tue, 12 Mar 2024 03:54:25 GMT   (2367kb,D)

Title: Graph Data Condensation via Self-expressive Graph Structure
  Reconstruction
Authors: Zhanyu Liu, Chaolv Zeng, Guanjie Zheng
Categories: cs.LG cs.AI cs.SI
\\
  With the increasing demands of training graph neural networks (GNNs) on
large-scale graphs, graph data condensation has emerged as a critical technique
to relieve the storage and time costs during the training phase. It aims to
condense the original large-scale graph to a much smaller synthetic graph while
preserving the essential information necessary for efficiently training a
downstream GNN. However, existing methods concentrate either on optimizing node
features exclusively or endeavor to independently learn node features and the
graph structure generator. They could not explicitly leverage the information
of the original graph structure and failed to construct an interpretable graph
structure for the synthetic dataset. To address these issues, we introduce a
novel framework named \textbf{G}raph Data \textbf{C}ondensation via
\textbf{S}elf-expressive Graph Structure \textbf{R}econstruction
(\textbf{GCSR}). Our method stands out by (1) explicitly incorporating the
original graph structure into the condensing process and (2) capturing the
nuanced interdependencies between the condensed nodes by reconstructing an
interpretable self-expressive graph structure. Extensive experiments and
comprehensive analysis validate the efficacy of the proposed method across
diverse GNN models and datasets. Our code is available at
https://www.dropbox.com/scl/fi/2aonyp5ln5gisdqtjimu8/GCSR.zip?rlkey=11cuwfpsf54wxiiktu0klud0x&dl=0
\\ ( https://arxiv.org/abs/2403.07294 ,  2367kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07308 (*cross-listing*)
Date: Tue, 12 Mar 2024 04:29:43 GMT   (1696kb,D)

Title: Verification-Aided Learning of Neural Network Barrier Functions with
  Termination Guarantees
Authors: Shaoru Chen, Lekan Molu, Mahyar Fazlyab
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: This is an online extended version of the same paper accepted to
  American Control Conference 2024
\\
  Barrier functions are a general framework for establishing a safety guarantee
for a system. However, there is no general method for finding these functions.
To address this shortcoming, recent approaches use self-supervised learning
techniques to learn these functions using training data that are periodically
generated by a verification procedure, leading to a verification-aided learning
framework. Despite its immense potential in automating barrier function
synthesis, the verification-aided learning framework does not have termination
guarantees and may suffer from a low success rate of finding a valid barrier
function in practice. In this paper, we propose a holistic approach to address
these drawbacks. With a convex formulation of the barrier function synthesis,
we propose to first learn an empirically well-behaved NN basis function and
then apply a fine-tuning algorithm that exploits the convexity and
counterexamples from the verification failure to find a valid barrier function
with finite-step termination guarantees: if there exist valid barrier
functions, the fine-tuning algorithm is guaranteed to find one in a finite
number of iterations. We demonstrate that our fine-tuning method can
significantly boost the performance of the verification-aided learning
framework on examples of different scales and using various neural network
verifiers.
\\ ( https://arxiv.org/abs/2403.07308 ,  1696kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07309 (*cross-listing*)
Date: Tue, 12 Mar 2024 04:36:41 GMT   (6299kb,D)

Title: Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM
  Framework with Mortality Classifier and Transformer
Authors: Dipesh Tamboli and Jiayu Chen and Kiran Pranesh Jotheeswaran and Denny
  Yu and Vaneet Aggarwal
Categories: cs.LG cs.AI cs.CY
Comments: Accepted to IEEE Journal of Biomedical and Health Informatics, Mar
  2024
\\
  Sepsis, a life-threatening condition triggered by the body's exaggerated
response to infection, demands urgent intervention to prevent severe
complications. Existing machine learning methods for managing sepsis struggle
in offline scenarios, exhibiting suboptimal performance with survival rates
below 50%. This paper introduces the POSNEGDM -- ``Reinforcement Learning with
Positive and Negative Demonstrations for Sequential Decision-Making" framework
utilizing an innovative transformer-based model and a feedback reinforcer to
replicate expert actions while considering individual patient characteristics.
A mortality classifier with 96.7\% accuracy guides treatment decisions towards
positive outcomes. The POSNEGDM framework significantly improves patient
survival, saving 97.39% of patients, outperforming established machine learning
algorithms (Decision Transformer and Behavioral Cloning) with survival rates of
33.4% and 43.5%, respectively. Additionally, ablation studies underscore the
critical role of the transformer-based decision maker and the integration of a
mortality classifier in enhancing overall survival rates. In summary, our
proposed approach presents a promising avenue for enhancing sepsis treatment
outcomes, contributing to improved patient care and reduced healthcare costs.
\\ ( https://arxiv.org/abs/2403.07309 ,  6299kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07322 (*cross-listing*)
Date: Tue, 12 Mar 2024 05:15:42 GMT   (2001kb,D)

Title: A Question-centric Multi-experts Contrastive Learning Framework for
  Improving the Accuracy and Interpretability of Deep Sequential Knowledge
  Tracing Models
Authors: Hengyuan Zhang, Zitao Liu, Chenming Shang, Dawei Li, Yong Jiang
Categories: cs.CY cs.AI cs.LG
Comments: 24 pages, 8 figures
\\
  Knowledge tracing (KT) plays a crucial role in predicting students' future
performance by analyzing their historical learning processes. Deep neural
networks (DNNs) have shown great potential in solving the KT problem. However,
there still exist some important challenges when applying deep learning
techniques to model the KT process. The first challenge lies in taking the
individual information of the question into modeling. This is crucial because,
despite questions sharing the same knowledge component (KC), students'
knowledge acquisition on homogeneous questions can vary significantly. The
second challenge lies in interpreting the prediction results from existing deep
learning-based KT models. In real-world applications, while it may not be
necessary to have complete transparency and interpretability of the model
parameters, it is crucial to present the model's prediction results in a manner
that teachers find interpretable. This makes teachers accept the rationale
behind the prediction results and utilize them to design teaching activities
and tailored learning strategies for students. However, the inherent black-box
nature of deep learning techniques often poses a hurdle for teachers to fully
embrace the model's prediction results. To address these challenges, we propose
a Question-centric Multi-experts Contrastive Learning framework for KT called
Q-MCKT.
\\ ( https://arxiv.org/abs/2403.07322 ,  2001kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07332 (*cross-listing*)
Date: Tue, 12 Mar 2024 05:34:51 GMT   (2121kb,D)

Title: Large Window-based Mamba UNet for Medical Image Segmentation: Beyond
  Convolution and Self-attention
Authors: Jinhong Wang, Jintai Chen, Danny Chen and Jian Wu
Categories: cs.CV cs.AI
\\
  In clinical practice, medical image segmentation provides useful information
on the contours and dimensions of target organs or tissues, facilitating
improved diagnosis, analysis, and treatment. In the past few years,
convolutional neural networks (CNNs) and Transformers have dominated this area,
but they still suffer from either limited receptive fields or costly long-range
modeling. Mamba, a State Space Sequence Model (SSM), recently emerged as a
promising paradigm for long-range dependency modeling with linear complexity.
In this paper, we introduce a Large Window-based Mamba U}-shape Network, or
LMa-UNet, for 2D and 3D medical image segmentation. A distinguishing feature of
our LMa-UNet is its utilization of large windows, excelling in locally spatial
modeling compared to small kernel-based CNNs and small window-based
Transformers, while maintaining superior efficiency in global modeling compared
to self-attention with quadratic complexity. Additionally, we design a novel
hierarchical and bidirectional Mamba block to further enhance the global and
neighborhood spatial modeling capability of Mamba. Comprehensive experiments
demonstrate the effectiveness and efficiency of our method and the feasibility
of using large window size to achieve large receptive fields. Codes are
available at https://github.com/wjh892521292/LMa-UNet.
\\ ( https://arxiv.org/abs/2403.07332 ,  2121kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07342 (*cross-listing*)
Date: Tue, 12 Mar 2024 06:01:04 GMT   (2887kb,D)

Title: Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive
  Learning
Authors: Qiao Sun, Liujia Yang, Minghao Ma, Nanyang Ye, Qinying Gu
Categories: cs.CL cs.AI
\\
  Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of
fine-grained sentiment analysis, aiming to extract structured sentiment
triplets from unstructured textual data. Existing approaches to ASTE often
complicate the task with additional structures or external data. In this
research, we propose a novel tagging scheme and employ a contrastive learning
approach to mitigate these challenges. The proposed approach demonstrates
comparable or superior performance in comparison to state-of-the-art
techniques, while featuring a more compact design and reduced computational
overhead. Notably, even in the era of Large Language Models (LLMs), our method
exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning
scenarios. This study also provides valuable insights for the advancement of
ASTE techniques within the paradigm of large language models.
\\ ( https://arxiv.org/abs/2403.07342 ,  2887kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07350 (*cross-listing*)
Date: Tue, 12 Mar 2024 06:16:33 GMT   (254kb)

Title: KEBench: A Benchmark on Knowledge Editing for Large Vision-Language
  Models
Authors: Han Huang, Haitian Zhong, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan
Categories: cs.CL cs.AI cs.CV
Comments: 13 pages
\\
  Currently, little research has been done on knowledge editing for Large
Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of
effectively integrating diverse modalities (image and text) while ensuring
coherent and contextually relevant modifications. An existing benchmark has
three metrics (Reliability, Locality and Generality) to measure knowledge
editing for LVLMs. However, the benchmark falls short in the quality of
generated images used in evaluation and cannot assess whether models
effectively utilize edited knowledge in relation to the associated content. We
adopt different data collection methods to construct a new benchmark,
$\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive
evaluation. Leveraging a multimodal knowledge graph, our image data exhibits
clear directionality towards entities. This directional aspect can be further
utilized to extract entity-related knowledge and form editing data. We
conducted experiments of different editing methods on five LVLMs, and
thoroughly analyze how these methods impact the models. The results reveal
strengths and deficiencies of these methods and, hopefully, provide insights
into potential avenues for future research.
\\ ( https://arxiv.org/abs/2403.07350 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07355 (*cross-listing*)
Date: Tue, 12 Mar 2024 06:28:41 GMT   (811kb)

Title: Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO
  Systems
Authors: Junyong Shin, Yujin Kang, Yo-Seb Jeon
Categories: eess.SP cs.AI cs.CV
\\
  This paper presents a finite-rate deep-learning (DL)-based channel state
information (CSI) feedback method for massive multiple-input multiple-output
(MIMO) systems. The presented method provides a finite-bit representation of
the latent vector based on a vector-quantized variational autoencoder (VQ-VAE)
framework while reducing its computational complexity based on shape-gain
vector quantization. In this method, the magnitude of the latent vector is
quantized using a non-uniform scalar codebook with a proper transformation
function, while the direction of the latent vector is quantized using a
trainable Grassmannian codebook. A multi-rate codebook design strategy is also
developed by introducing a codeword selection rule for a nested codebook along
with the design of a loss function. Simulation results demonstrate that the
proposed method reduces the computational complexity associated with VQ-VAE
while improving CSI reconstruction performance under a given feedback overhead.
\\ ( https://arxiv.org/abs/2403.07355 ,  811kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07362 (*cross-listing*)
Date: Tue, 12 Mar 2024 06:50:32 GMT   (27366kb,D)

Title: Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine
  Unlearning
Authors: Chongyu Fan, Jiancheng Liu, Alfred Hero, Sijia Liu
Categories: cs.LG cs.AI cs.CV
\\
  The trustworthy machine learning (ML) community is increasingly recognizing
the crucial need for models capable of selectively 'unlearning' data points
after training. This leads to the problem of machine unlearning (MU), aiming to
eliminate the influence of chosen data points on model performance, while still
maintaining the model's utility post-unlearning. Despite various MU methods for
data influence erasure, evaluations have largely focused on random data
forgetting, ignoring the vital inquiry into which subset should be chosen to
truly gauge the authenticity of unlearning performance. To tackle this issue,
we introduce a new evaluative angle for MU from an adversarial viewpoint. We
propose identifying the data subset that presents the most significant
challenge for influence erasure, i.e., pinpointing the worst-case forget set.
Utilizing a bi-level optimization principle, we amplify unlearning challenges
at the upper optimization level to emulate worst-case scenarios, while
simultaneously engaging in standard training and unlearning at the lower level,
achieving a balance between data influence erasure and model utility. Our
proposal offers a worst-case evaluation of MU's resilience and effectiveness.
Through extensive experiments across different datasets (including CIFAR-10,
100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image
classifiers and generative models), we expose critical pros and cons in
existing (approximate) unlearning strategies. Our results illuminate the
complex challenges of MU in practice, guiding the future development of more
accurate and robust unlearning algorithms. The code is available at
https://github.com/OPTML-Group/Unlearn-WorstCase.
\\ ( https://arxiv.org/abs/2403.07362 ,  27366kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07376 (*cross-listing*)
Date: Tue, 12 Mar 2024 07:27:02 GMT   (15541kb,D)

Title: NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning
  Disentangled Reasoning
Authors: Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma,
  Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang
Categories: cs.CV cs.AI cs.CL cs.RO
\\
  Vision-and-Language Navigation (VLN), as a crucial research problem of
Embodied AI, requires an embodied agent to navigate through complex 3D
environments following natural language instructions. Recent research has
highlighted the promising capacity of large language models (LLMs) in VLN by
improving navigational reasoning accuracy and interpretability. However, their
predominant use in an offline manner usually suffers from substantial domain
gap between the VLN task and the LLM training corpus. This paper introduces a
novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill
parameter-efficient in-domain training to enable self-guided navigational
decision, leading to a significant mitigation of the domain gap in a
cost-effective manner. Specifically, at each timestep, the LLM is prompted to
forecast the navigational chain-of-thought by: 1) acting as a world model to
imagine the next observation according to the instruction, 2) selecting the
candidate observation that best aligns with the imagination, and 3) determining
the action based on the reasoning from the prior steps. Through constructing
formalized labels for training, the LLM can learn to generate desired and
reasonable chain-of-thought outputs for improving the action decision.
Experimental results across various training settings and popular VLN
benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room
(R4R)) show the significant superiority of NavCoT over the direct action
prediction variants. Through simple parameter-efficient finetuning, our NavCoT
outperforms a recent GPT4-based approach with ~7% relative improvement on the
R2R dataset. We believe that NavCoT will help unlock more task-adaptive and
scalable LLM-based embodied agents, which are helpful for developing real-world
robotics applications. Code is available at
https://github.com/expectorlin/NavCoT.
\\ ( https://arxiv.org/abs/2403.07376 ,  15541kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07380 (*cross-listing*)
Date: Tue, 12 Mar 2024 07:41:51 GMT   (466kb,D)

Title: Gabor-guided transformer for single image deraining
Authors: Sijin He, Guangfeng Lin
Categories: cs.CV cs.AI
\\
  Image deraining have have gained a great deal of attention in order to
address the challenges posed by the effects of harsh weather conditions on
visual tasks. While convolutional neural networks (CNNs) are popular, their
limitations in capturing global information may result in ineffective rain
removal. Transformer-based methods with self-attention mechanisms have
improved, but they tend to distort high-frequency details that are crucial for
image fidelity. To solve this problem, we propose the Gabor-guided tranformer
(Gabformer) for single image deraining. The focus on local texture features is
enhanced by incorporating the information processed by the Gabor filter into
the query vector, which also improves the robustness of the model to noise due
to the properties of the filter. Extensive experiments on the benchmarks
demonstrate that our method outperforms state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2403.07380 ,  466kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07384 (*cross-listing*)
Date: Tue, 12 Mar 2024 07:45:33 GMT   (195kb,D)

Title: SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large
  Language Models by Summarizing Training Trajectories of Small Models
Authors: Yu Yang, Siddhartha Mishra, Jeffrey N Chiang, Baharan Mirzasoleiman
Categories: cs.CL cs.AI cs.LG
\\
  Despite the effectiveness of data selection for large language models (LLMs)
during pretraining and instruction fine-tuning phases, improving data
efficiency in supervised fine-tuning (SFT) for specialized domains poses
significant challenges due to the complexity of fine-tuning data. To bridge
this gap, we introduce an effective and scalable data selection method for SFT,
SmallToLarge (S2L), which leverages training trajectories from small models to
guide the data selection for larger models. We demonstrate through extensive
experiments that S2L significantly improves data efficiency in SFT for
mathematical problem-solving, reducing the training data to just 11% of the
original MathInstruct dataset (Yue et al., 2023) to match full dataset
performance while outperforming state-of-the-art data selection algorithms by
an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,
selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most
challenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et
al., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset
(Johnson et al., 2016), S2L again outperforms training on the full dataset
using only 50% of the data. Notably, S2L can perform data selection using a
reference model 40x smaller than the target model, proportionally reducing the
cost of data selection.
\\ ( https://arxiv.org/abs/2403.07384 ,  195kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07389 (*cross-listing*)
Date: Tue, 12 Mar 2024 07:57:33 GMT   (6634kb,D)

Title: Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from
  Duplex to Monoplex IHC Images
Authors: Nicolas Brieu, Nicolas Triltsch, Philipp Wortmann, Dominik Winter,
  Shashank Saran, Marlon Rebelatto, G\"unter Schmidt
Categories: cs.CV cs.AI eess.IV
Comments: 4 pages, 2 figures
MSC-class: I.2.10, J.3, I.4.6
\\
  Generative models enable the translation from a source image domain where
readily trained models are available to a target domain unseen during training.
While Cycle Generative Adversarial Networks (GANs) are well established, the
associated cycle consistency constrain relies on that an invertible mapping
exists between the two domains. This is, however, not the case for the
translation between images stained with chromogenic monoplex and duplex
immunohistochemistry (IHC) assays. Focusing on the translation from the latter
to the first, we propose - through the introduction of a novel training design,
an alternative constrain leveraging a set of immunofluorescence (IF) images as
an auxiliary unpaired image domain. Quantitative and qualitative results on a
downstream segmentation task show the benefit of the proposed method in
comparison to baseline approaches.
\\ ( https://arxiv.org/abs/2403.07389 ,  6634kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07398 (*cross-listing*)
Date: Tue, 12 Mar 2024 08:13:52 GMT   (1010kb,D)

Title: Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs
Authors: Tianqing Fang, Zeming Chen, Yangqiu Song, Antoine Bosselut
Categories: cs.CL cs.AI
Comments: 19 pages
\\
  Event commonsense reasoning requires the ability to reason about the
relationship between events, as well as infer implicit context underlying that
relationship. However, data scarcity makes it challenging for language models
to learn to generate commonsense inferences for contexts and questions
involving interactions between complex events. To address this demand, we
present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop
logical queries (e.g., the joint effect or cause of both event A and B, or the
effect of the effect of event C) from an existing commonsense knowledge graph
(CSKG), and verbalizing them using handcrafted rules and large language models
into multiple-choice and text generation questions. Our experiments show that
language models trained on COM2 exhibit significant improvements in complex
reasoning ability, resulting in enhanced zero-shot performance in both
in-domain and out-of-domain tasks for question answering and generative
commonsense reasoning, without expensive human annotations.
\\ ( https://arxiv.org/abs/2403.07398 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07403 (*cross-listing*)
Date: Tue, 12 Mar 2024 08:32:23 GMT   (18285kb,D)

Title: From Canteen Food to Daily Meals: Generalizing Food Recognition to More
  Practical Scenarios
Authors: Guoshan Liu, Yang Jiao, Jingjing Chen, Bin Zhu, Yu-Gang Jiang
Categories: cs.CV cs.AI
\\
  The precise recognition of food categories plays a pivotal role for
intelligent health management, attracting significant research attention in
recent years. Prominent benchmarks, such as Food-101 and VIREO Food-172,
provide abundant food image resources that catalyze the prosperity of research
in this field. Nevertheless, these datasets are well-curated from canteen
scenarios and thus deviate from food appearances in daily life. This
discrepancy poses great challenges in effectively transferring classifiers
trained on these canteen datasets to broader daily-life scenarios encountered
by humans. Toward this end, we present two new benchmarks, namely DailyFood-172
and DailyFood-16, specifically designed to curate food images from everyday
meals. These two datasets are used to evaluate the transferability of
approaches from the well-curated food image domain to the everyday-life food
image domain. In addition, we also propose a simple yet effective baseline
method named Multi-Cluster Reference Learning (MCRL) to tackle the
aforementioned domain gap. MCRL is motivated by the observation that food
images in daily-life scenarios exhibit greater intra-class appearance variance
compared with those in well-curated benchmarks. Notably, MCRL can be seamlessly
coupled with existing approaches, yielding non-trivial performance
enhancements. We hope our new benchmarks can inspire the community to explore
the transferability of food recognition models trained on well-curated datasets
toward practical real-life applications.
\\ ( https://arxiv.org/abs/2403.07403 ,  18285kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07404 (*cross-listing*)
Date: Tue, 12 Mar 2024 08:33:26 GMT   (3603kb,D)

Title: Accelerated Inference and Reduced Forgetting: The Dual Benefits of
  Early-Exit Networks in Continual Learning
Authors: Filip Szatkowski, Fei Yang, Bart{\l}omiej Twardowski, Tomasz
  Trzci\'nski, Joost van de Weijer
Categories: cs.LG cs.AI
\\
  Driven by the demand for energy-efficient employment of deep neural networks,
early-exit methods have experienced a notable increase in research attention.
These strategies allow for swift predictions by making decisions early in the
network, thereby conserving computation time and resources. However, so far the
early-exit networks have only been developed for stationary data distributions,
which restricts their application in real-world scenarios with continuous
non-stationary data. This study aims to explore the continual learning of the
early-exit networks. We adapt existing continual learning methods to fit with
early-exit architectures and investigate their behavior in the continual
setting. We notice that early network layers exhibit reduced forgetting and can
outperform standard networks even when using significantly fewer resources.
Furthermore, we analyze the impact of task-recency bias on early-exit inference
and propose Task-wise Logits Correction (TLC), a simple method that equalizes
this bias and improves the network performance for every given compute budget
in the class-incremental setting. We assess the accuracy and computational cost
of various continual learning techniques enhanced with early-exits and TLC
across standard class-incremental learning benchmarks such as 10 split CIFAR100
and ImageNetSubset and show that TLC can achieve the accuracy of the standard
methods using less than 70\% of their computations. Moreover, at full
computational budget, our method outperforms the accuracy of the standard
counterparts by up to 15 percentage points. Our research underscores the
inherent synergy between early-exit networks and continual learning,
emphasizing their practical utility in resource-constrained environments.
\\ ( https://arxiv.org/abs/2403.07404 ,  3603kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07440 (*cross-listing*)
Date: Tue, 12 Mar 2024 09:32:25 GMT   (1122kb,D)

Title: Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A
  Brain-Inspired Method for Parameter-Efficient Fine-Tuning
Authors: Yao Liang, Yuwei Wang, Yi Zeng
Categories: cs.CL cs.AI
\\
  Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have
been proven to significantly enhance model performance on a variety of
downstream tasks and effectively control the output behaviors of LPLMs. Recent
studies have proposed numerous methods for fine-tuning a small number of
parameters based on open-source LPLMs, reducing the demand for computational
and storage resources. Among these, reparameterization fine-tuning methods
represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that
although these methods perform well in many aspects, there is still
considerable room for improvement in terms of complex task adaptability,
performance, stability, and algorithm complexity. In response to this, inspired
by the idea that the functions of the brain are shaped by its geometric
structure, this paper integrates this idea into LoRA technology and proposes a
new matrix transformation-based reparameterization method for efficient
fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).
MTLoRA aims to dynamically alter its spatial geometric structure by applying a
transformation-matrix T to perform linear transformations, such as rotation,
scaling, and translation, on the task-specific parameter matrix, generating new
matrix feature patterns (eigenvectors) to mimic the fundamental influence of
complex geometric structure feature patterns in the brain on functions, thereby
enhancing the model's performance in downstream tasks. In Natural Language
Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and
the results reveal that MTLoRA achieves an overall performance increase of
about 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,
MTLoRA improves performance by an average of 0.95% and 0.31% in the DART and
WebNLG tasks, respectively.
\\ ( https://arxiv.org/abs/2403.07440 ,  1122kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07483 (*cross-listing*)
Date: Tue, 12 Mar 2024 10:18:59 GMT   (574kb,D)

Title: A Deep Learning Approach to Diabetes Diagnosis
Authors: Zeyu Zhang, Khandaker Asif Ahmed, Md Rakibul Hasan, Tom Gedeon, Md
  Zakir Hossain
Categories: cs.LG cs.AI
Comments: Accepted to ACIIDS 2024
\\
  Diabetes, resulting from inadequate insulin production or utilization, causes
extensive harm to the body. Existing diagnostic methods are often invasive and
come with drawbacks, such as cost constraints. Although there are machine
learning models like Classwise k Nearest Neighbor (CkNN) and General Regression
Neural Network (GRNN), they struggle with imbalanced data and result in
under-performance. Leveraging advancements in sensor technology and machine
learning, we propose a non-invasive diabetes diagnosis using a Back Propagation
Neural Network (BPNN) with batch normalization, incorporating data re-sampling
and normalization for class balancing. Our method addresses existing challenges
such as limited performance associated with traditional machine learning.
Experimental results on three datasets show significant improvements in overall
accuracy, sensitivity, and specificity compared to traditional methods.
Notably, we achieve accuracies of 89.81% in Pima diabetes dataset, 75.49% in
CDC BRFSS2015 dataset, and 95.28% in Mesra Diabetes dataset. This underscores
the potential of deep learning models for robust diabetes diagnosis. See
project website https://steve-zeyu-zhang.github.io/DiabetesDiagnosis/
\\ ( https://arxiv.org/abs/2403.07483 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07500 (*cross-listing*)
Date: Tue, 12 Mar 2024 10:38:03 GMT   (8102kb,D)

Title: Block-wise LoRA: Revisiting Fine-grained LoRA for Effective
  Personalization and Stylization in Text-to-Image Generation
Authors: Likun Li, Haoqi Zeng, Changpeng Yang, Haozhe Jia, Di Xu
Categories: cs.CV cs.AI
\\
  The objective of personalization and stylization in text-to-image is to
instruct a pre-trained diffusion model to analyze new concepts introduced by
users and incorporate them into expected styles. Recently, parameter-efficient
fine-tuning (PEFT) approaches have been widely adopted to address this task and
have greatly propelled the development of this field. Despite their popularity,
existing efficient fine-tuning methods still struggle to achieve effective
personalization and stylization in T2I generation. To address this issue, we
propose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained
fine-tuning for different blocks of SD, which can generate images faithful to
input prompts and target identity and also with desired style. Extensive
experiments demonstrate the effectiveness of the proposed method.
\\ ( https://arxiv.org/abs/2403.07500 ,  8102kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07540 (*cross-listing*)
Date: Tue, 12 Mar 2024 11:26:58 GMT   (6083kb,D)

Title: WannaLaugh: A Configurable Ransomware Emulator -- Learning to Mimic
  Malicious Storage Traces
Authors: Dionysios Diamantopolous and Roman Pletka and Slavisa Sarafijanovic
  and A.L. Narasimha Reddy and Haris Pozidis
Categories: cs.CR cs.AI
\\
  Ransomware, a fearsome and rapidly evolving cybersecurity threat, continues
to inflict severe consequences on individuals and organizations worldwide.
Traditional detection methods, reliant on static signatures and application
behavioral patterns, are challenged by the dynamic nature of these threats.
This paper introduces three primary contributions to address this challenge.
First, we introduce a ransomware emulator. This tool is designed to safely
mimic ransomware attacks without causing actual harm or spreading malware,
making it a unique solution for studying ransomware behavior. Second, we
demonstrate how we use this emulator to create storage I/O traces. These traces
are then utilized to train machine-learning models. Our results show that these
models are effective in detecting ransomware, highlighting the practical
application of our emulator in developing responsible cybersecurity tools.
Third, we show how our emulator can be used to mimic the I/O behavior of
existing ransomware thereby enabling safe trace collection. Both the emulator
and its application represent significant steps forward in ransomware detection
in the era of machine-learning-driven cybersecurity.
\\ ( https://arxiv.org/abs/2403.07540 ,  6083kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07553 (*cross-listing*)
Date: Tue, 12 Mar 2024 11:39:18 GMT   (3692kb,D)

Title: The future of document indexing: GPT and Donut revolutionize table of
  content processing
Authors: Degaga Wolde Feyisa, Haylemicheal Berihun, Amanuel Zewdu, Mahsa
  Najimoghadam, Marzieh Zare
Categories: cs.IR cs.AI cs.CV
Comments: Document AI, Document Classification, Information extraction, Large
  Language Models, OCR Models, Visual Document Understanding
\\
  Industrial projects rely heavily on lengthy, complex specification documents,
making tedious manual extraction of structured information a major bottleneck.
This paper introduces an innovative approach to automate this process,
leveraging the capabilities of two cutting-edge AI models: Donut, a model that
extracts information directly from scanned documents without OCR, and OpenAI
GPT-3.5 Turbo, a robust large language model. The proposed methodology is
initiated by acquiring the table of contents (ToCs) from construction
specification documents and subsequently structuring the ToCs text into JSON
data. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5
Turbo reaching 89% in effectively organizing the ToCs. This landmark
achievement represents a significant leap forward in document indexing,
demonstrating the immense potential of AI to automate information extraction
tasks across diverse document types, boosting efficiency and liberating
critical resources in various industries.
\\ ( https://arxiv.org/abs/2403.07553 ,  3692kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07559 (*cross-listing*)
Date: Tue, 12 Mar 2024 11:47:12 GMT   (771kb,D)

Title: Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding
Authors: Huijie Tang, Federico Berto, Jinkyoo Park
Categories: cs.MA cs.AI cs.LG cs.RO
\\
  Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding
(MAPF) has recently gained attention due to its efficiency and scalability.
Several MARL-MAPF methods choose to use communication to enrich the information
one agent can perceive. However, existing works still struggle in structured
environments with high obstacle density and a high number of agents. To further
improve the performance of the communication-based MARL-MAPF solvers, we
propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first
propose a selective communication block to gather richer information for better
agent coordination within multi-agent environments and train the model with a
Q-learning-based algorithm. We further introduce three advanced inference
strategies aimed at bolstering performance during the execution phase. First,
we hybridize the neural policy with single-agent expert guidance for navigating
conflict-free zones. Secondly, we propose Q value-based methods for prioritized
resolution of conflicts as well as deadlock situations. Finally, we introduce a
robust ensemble method that can efficiently collect the best out of multiple
possible solutions. We empirically evaluate EPH in complex multi-agent
environments and demonstrate competitive performance against state-of-the-art
neural methods for MAPF.
\\ ( https://arxiv.org/abs/2403.07559 ,  771kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07573 (*cross-listing*)
Date: Tue, 12 Mar 2024 12:03:16 GMT   (6941kb,D)

Title: Towards a Dynamic Future with Adaptable Computing and Network
  Convergence (ACNC)
Authors: Masoud Shokrnezhad, Hao Yu, Tarik Taleb, Richard Li, Kyunghan Lee,
  Jaeseung Song, and Cedric Westphal
Categories: cs.NI cs.AI cs.DC cs.ET cs.LG
\\
  In the context of advancing 6G, a substantial paradigm shift is anticipated,
highlighting comprehensive everything-to-everything interactions characterized
by numerous connections and stringent adherence to Quality of
Service/Experience (QoS/E) prerequisites. The imminent challenge stems from
resource scarcity, prompting a deliberate transition to Computing-Network
Convergence (CNC) as an auspicious approach for joint resource orchestration.
While CNC-based mechanisms have garnered attention, their effectiveness in
realizing future services, particularly in use cases like the Metaverse, may
encounter limitations due to the continually changing nature of users,
services, and resources. Hence, this paper presents the concept of Adaptable
CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for
the joint orchestration of computing and network resources, catering to dynamic
and voluminous user requests with stringent requirements. ACNC encompasses two
primary functionalities: state recognition and context detection. Given the
intricate nature of the user-service-computing-network space, the paper employs
dimension reduction to generate live, holistic, abstract system states in a
hierarchical structure. To address the challenges posed by dynamic changes,
Continual Learning (CL) is employed, classifying the system state into contexts
controlled by dedicated ML agents, enabling them to operate efficiently. These
two functionalities are intricately linked within a closed loop overseen by the
End-to-End (E2E) orchestrator to allocate resources. The paper introduces the
components of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in
resource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow,
details a numerical analysis for efficiency assessment, and concludes with
discussions on relevant challenges and potential avenues for future research.
\\ ( https://arxiv.org/abs/2403.07573 ,  6941kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07586 (*cross-listing*)
Date: Tue, 12 Mar 2024 12:16:40 GMT   (1323kb,D)

Title: Federated Learning of Socially Appropriate Agent Behaviours in Simulated
  Home Environments
Authors: Saksham Checker and Nikhil Churamani and Hatice Gunes
Categories: cs.LG cs.AI cs.CY cs.RO
Comments: Accepted at the Workshop on Lifelong Learning and Personalization in
  Long-Term Human-Robot Interaction (LEAP-HRI) at the 19th ACM/IEEE
  International Conference on Human-Robot Interaction (HRI), 2024
\\
  As social robots become increasingly integrated into daily life, ensuring
their behaviours align with social norms is crucial. For their widespread
open-world application, it is important to explore Federated Learning (FL)
settings where individual robots can learn about their unique environments
while also learning from each others' experiences. In this paper, we present a
novel FL benchmark that evaluates different strategies, using multi-label
regression objectives, where each client individually learns to predict the
social appropriateness of different robot actions while also sharing their
learning with others. Furthermore, splitting the training data by different
contexts such that each client incrementally learns across contexts, we present
a novel Federated Continual Learning (FCL) benchmark that adapts FL-based
methods to use state-of-the-art Continual Learning (CL) methods to continually
learn socially appropriate agent behaviours under different contextual
settings. Federated Averaging (FedAvg) of weights emerges as a robust FL
strategy while rehearsal-based FCL enables incrementally learning the social
appropriateness of robot actions, across contextual splits.
\\ ( https://arxiv.org/abs/2403.07586 ,  1323kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07605 (*cross-listing*)
Date: Tue, 12 Mar 2024 12:44:34 GMT   (2312kb,D)

Title: Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in
  Text-To-Image Generation
Authors: Michael Ogezi and Ning Shi
Categories: cs.CV cs.AI cs.LG
\\
  In text-to-image generation, using negative prompts, which describe
undesirable image characteristics, can significantly boost image quality.
However, producing good negative prompts is manual and tedious. To address
this, we propose NegOpt, a novel method for optimizing negative prompt
generation toward enhanced image generation, using supervised fine-tuning and
reinforcement learning. Our combined approach results in a substantial increase
of 25% in Inception Score compared to other approaches and surpasses
ground-truth negative prompts from the test set. Furthermore, with NegOpt we
can preferentially optimize the metrics most important to us. Finally, we
construct Negative Prompts DB, a dataset of negative prompts.
\\ ( https://arxiv.org/abs/2403.07605 ,  2312kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07608 (*cross-listing*)
Date: Tue, 12 Mar 2024 12:47:32 GMT   (1012kb,D)

Title: Couler: Unified Machine Learning Workflow Optimization in Cloud
Authors: Xiaoda Wang, Yuan Tang, Tengda Guo, Bo Sang, Jingji Wu, Jian Sha, Ke
  Zhang, Jiang Qian, Mingjie Tang
Categories: cs.DB cs.AI cs.LG
\\
  Machine Learning (ML) has become ubiquitous, fueling data-driven applications
across various organizations. Contrary to the traditional perception of ML in
research, ML workflows can be complex, resource-intensive, and time-consuming.
Expanding an ML workflow to encompass a wider range of data infrastructure and
data types may lead to larger workloads and increased deployment costs.
Currently, numerous workflow engines are available (with over ten being widely
recognized). This variety poses a challenge for end-users in terms of mastering
different engine APIs. While efforts have primarily focused on optimizing ML
Operations (MLOps) for a specific workflow engine, current methods largely
overlook workflow optimization across different engines.
  In this work, we design and implement Couler, a system designed for unified
ML workflow optimization in the cloud. Our main insight lies in the ability to
generate an ML workflow using natural language (NL) descriptions. We integrate
Large Language Models (LLMs) into workflow generation, and provide a unified
programming interface for various workflow engines. This approach alleviates
the need to understand various workflow engines' APIs. Moreover, Couler
enhances workflow computation efficiency by introducing automated caching at
multiple stages, enabling large workflow auto-parallelization and automatic
hyperparameters tuning. These enhancements minimize redundant computational
costs and improve fault tolerance during deep learning workflow training.
Couler is extensively deployed in real-world production scenarios at Ant Group,
handling approximately 22k workflows daily, and has successfully improved the
CPU/Memory utilization by more than 15% and the workflow completion rate by
around 17%.
\\ ( https://arxiv.org/abs/2403.07608 ,  1012kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07611 (*cross-listing*)
Date: Tue, 12 Mar 2024 12:49:47 GMT   (337kb,D)

Title: Efficient Knowledge Deletion from Trained Models through Layer-wise
  Partial Machine Unlearning
Authors: Vinay Chakravarthi Gogineni and Esmaeil S. Nadimi
Categories: cs.LG cs.AI
Comments: 16pages, 4 figures
\\
  Machine unlearning has garnered significant attention due to its ability to
selectively erase knowledge obtained from specific training data samples in an
already trained machine learning model. This capability enables data holders to
adhere strictly to data protection regulations. However, existing unlearning
techniques face practical constraints, often causing performance degradation,
demanding brief fine-tuning post unlearning, and requiring significant storage.
In response, this paper introduces a novel class of machine unlearning
algorithms. First method is partial amnesiac unlearning, integration of
layer-wise pruning with amnesiac unlearning. In this method, updates made to
the model during training are pruned and stored, subsequently used to forget
specific data from trained model. The second method assimilates layer-wise
partial-updates into label-flipping and optimization-based unlearning to
mitigate the adverse effects of data deletion on model efficacy. Through a
detailed experimental evaluation, we showcase the effectiveness of proposed
unlearning methods. Experimental results highlight that the partial amnesiac
unlearning not only preserves model efficacy but also eliminates the necessity
for brief post fine-tuning, unlike conventional amnesiac unlearning. Moreover,
employing layer-wise partial updates in label-flipping and optimization-based
unlearning techniques demonstrates superiority in preserving model efficacy
compared to their naive counterparts.
\\ ( https://arxiv.org/abs/2403.07611 ,  337kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07622 (*cross-listing*)
Date: Tue, 12 Mar 2024 13:05:51 GMT   (19627kb,D)

Title: Multiple Latent Space Mapping for Compressed Dark Image Enhancement
Authors: Yi Zeng, Zhengning Wang, Yuxuan Liu, Tianjiao Zeng, Xuhang Liu,
  Xinglong Luo, Shuaicheng Liu, Shuyuan Zhu and Bing Zeng
Categories: cs.CV cs.AI eess.IV
\\
  Dark image enhancement aims at converting dark images to normal-light images.
Existing dark image enhancement methods take uncompressed dark images as inputs
and achieve great performance. However, in practice, dark images are often
compressed before storage or transmission over the Internet. Current methods
get poor performance when processing compressed dark images. Artifacts hidden
in the dark regions are amplified by current methods, which results in
uncomfortable visual effects for observers. Based on this observation, this
study aims at enhancing compressed dark images while avoiding compression
artifacts amplification. Since texture details intertwine with compression
artifacts in compressed dark images, detail enhancement and blocking artifacts
suppression contradict each other in image space. Therefore, we handle the task
in latent space. To this end, we propose a novel latent mapping network based
on variational auto-encoder (VAE). Firstly, different from previous VAE-based
methods with single-resolution features only, we exploit multiple latent spaces
with multi-resolution features, to reduce the detail blur and improve image
fidelity. Specifically, we train two multi-level VAEs to project compressed
dark images and normal-light images into their latent spaces respectively.
Secondly, we leverage a latent mapping network to transform features from
compressed dark space to normal-light space. Specifically, since the
degradation models of darkness and compression are different from each other,
the latent mapping process is divided mapping into enlightening branch and
deblocking branch. Comprehensive experiments demonstrate that the proposed
method achieves state-of-the-art performance in compressed dark image
enhancement.
\\ ( https://arxiv.org/abs/2403.07622 ,  19627kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07630 (*cross-listing*)
Date: Tue, 12 Mar 2024 13:11:58 GMT   (6780kb,D)

Title: Hunting Attributes: Context Prototype-Aware Learning for Weakly
  Supervised Semantic Segmentation
Authors: Feilong Tang, Zhongxing Xu, Zhaojun Qu, Wei Feng, Xingjian Jiang,
  Zongyuan Ge
Categories: cs.CV cs.AI
\\
  Recent weakly supervised semantic segmentation (WSSS) methods strive to
incorporate contextual knowledge to improve the completeness of class
activation maps (CAM). In this work, we argue that the knowledge bias between
instances and contexts affects the capability of the prototype to sufficiently
understand instance semantics. Inspired by prototype learning theory, we
propose leveraging prototype awareness to capture diverse and fine-grained
feature attributes of instances. The hypothesis is that contextual prototypes
might erroneously activate similar and frequently co-occurring object
categories due to this knowledge bias. Therefore, we propose to enhance the
prototype representation ability by mitigating the bias to better capture
spatial coverage in semantic object regions. With this goal, we present a
Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic
context to enrich instance comprehension. The core of this method is to
accurately capture intra-class variations in object features through
context-aware prototypes, facilitating the adaptation to the semantic
attributes of various instances. We design feature distribution alignment to
optimize prototype awareness, aligning instance feature distributions with
dense features. In addition, a unified training framework is proposed to
combine label-guided classification supervision and prototypes-guided
self-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show
that CPAL significantly improves off-the-shelf methods and achieves
state-of-the-art performance. The project is available at
https://github.com/Barrett-python/CPAL.
\\ ( https://arxiv.org/abs/2403.07630 ,  6780kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07657 (*cross-listing*)
Date: Tue, 12 Mar 2024 13:47:50 GMT   (11758kb,D)

Title: Scalable Spatiotemporal Prediction with Bayesian Neural Fields
Authors: Feras Saad, Jacob Burnim, Colin Carroll, Brian Patton, Urs K\"oster,
  Rif A. Saurous, Matthew Hoffman
Categories: cs.LG cs.AI stat.AP stat.ME
Comments: 22 pages, 6 figures, 3 tables
\\
  Spatiotemporal datasets, which consist of spatially-referenced time series,
are ubiquitous in many scientific and business-intelligence applications, such
as air pollution monitoring, disease tracking, and cloud-demand forecasting. As
modern datasets continue to increase in size and complexity, there is a growing
need for new statistical methods that are flexible enough to capture complex
spatiotemporal dynamics and scalable enough to handle large prediction
problems. This work presents the Bayesian Neural Field (BayesNF), a
domain-general statistical model for inferring rich probability distributions
over a spatiotemporal domain, which can be used for data-analysis tasks
including forecasting, interpolation, and variography. BayesNF integrates a
novel deep neural network architecture for high-capacity function estimation
with hierarchical Bayesian inference for robust uncertainty quantification. By
defining the prior through a sequence of smooth differentiable transforms,
posterior inference is conducted on large-scale data using variationally
learned surrogates trained via stochastic gradient descent. We evaluate BayesNF
against prominent statistical and machine-learning baselines, showing
considerable improvements on diverse prediction problems from climate and
public health datasets that contain tens to hundreds of thousands of
measurements. The paper is accompanied with an open-source software package
(https://github.com/google/bayesnf) that is easy-to-use and compatible with
modern GPU and TPU accelerators on the JAX machine learning platform.
\\ ( https://arxiv.org/abs/2403.07657 ,  11758kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07687 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:27:17 GMT   (11363kb,D)

Title: Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model
  Performance and Annotation Cost
Authors: Oana Ignat, Longju Bai, Joan Nwatu, Rada Mihalcea
Categories: cs.CV cs.AI cs.CL
Comments: accepted at COLING 2024
\\
  Current foundation models have shown impressive performance across various
tasks. However, several studies have revealed that these models are not
effective for everyone due to the imbalanced geographical and economic
representation of the data used in the training process. Most of this data
comes from Western countries, leading to poor results for underrepresented
countries. To address this issue, more data needs to be collected from these
countries, but the cost of annotation can be a significant bottleneck. In this
paper, we propose methods to identify the data to be annotated to balance model
performance and annotation costs. Our approach first involves finding the
countries with images of topics (objects and actions) most visually distinct
from those already in the training datasets used by current large
vision-language foundation models. Next, we identify countries with higher
visual similarity for these topics and show that using data from these
countries to supplement the training data improves model performance and
reduces annotation costs. The resulting lists of countries and corresponding
topics are made available at
https://github.com/MichiganNLP/visual_diversity_budget.
\\ ( https://arxiv.org/abs/2403.07687 ,  11363kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07688 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:28:06 GMT   (1217kb,D)

Title: Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of
  Neurons
Authors: Simon Dufort-Labb\'e, Pierluca D'Oro, Evgenii Nikishin, Razvan
  Pascanu, Pierre-Luc Bacon, Aristide Baratin
Categories: cs.LG cs.AI
\\
  When training deep neural networks, the phenomenon of $\textit{dying
neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero
during training$\unicode{x2013}$ has traditionally been viewed as undesirable,
linked with optimization challenges, and contributing to plasticity loss in
continual learning scenarios. In this paper, we reassess this phenomenon,
focusing on sparsity and pruning. By systematically exploring the impact of
various hyperparameter configurations on dying neurons, we unveil their
potential to facilitate simple yet effective structured pruning algorithms. We
introduce $\textit{Demon Pruning}$ (DemP), a method that controls the
proliferation of dead neurons, dynamically leading to network sparsity.
Achieved through a combination of noise injection on active units and a
one-cycled schedule regularization strategy, DemP stands out for its simplicity
and broad applicability. Experiments on CIFAR10 and ImageNet datasets
demonstrate that DemP surpasses existing structured pruning techniques,
showcasing superior accuracy-sparsity tradeoffs and training speedups. These
findings suggest a novel perspective on dying neurons as a valuable resource
for efficient model compression and optimization.
\\ ( https://arxiv.org/abs/2403.07688 ,  1217kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07691 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:34:08 GMT   (8428kb,D)

Title: Reference-free Monolithic Preference Optimization with Odds Ratio
Authors: Jiwoo Hong, Noah Lee, James Thorne
Categories: cs.CL cs.AI
Comments: Preprint
\\
  While recent preference alignment algorithms for language models have
demonstrated promising results, supervised fine-tuning (SFT) remains imperative
for achieving successful convergence. In this paper, we study the crucial role
of SFT within the context of preference alignment, emphasizing that a minor
penalty for the disfavored generation style is sufficient for
preference-aligned SFT. Building on this foundation, we introduce a
straightforward and innovative reference model-free monolithic odds ratio
preference optimization algorithm, ORPO, eliminating the necessity for an
additional preference alignment phase. We demonstrate, both empirically and
theoretically, that the odds ratio is a sensible choice for contrasting favored
and disfavored styles during SFT across the diverse sizes from 125M to 7B.
Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with
ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art
language models with more than 7B and 13B parameters: achieving up to 12.20% on
$\text{AlpacaEval}_{2.0}$ and 7.32 in MT-Bench, as shown in Figures 1 and 12.
We release code and model checkpoints for Mistral-ORPO-$\alpha$ (7B) and
Mistral-ORPO-$\beta$ (7B).
\\ ( https://arxiv.org/abs/2403.07691 ,  8428kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07693 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:37:03 GMT   (1507kb,D)

Title: Large, Small or Both: A Novel Data Augmentation Framework Based on
  Language Models for Debiasing Opinion Summarization
Authors: Yanyue Zhang, Pengfei Li, Yilong Lai and Deyu Zhou
Categories: cs.CL cs.AI
\\
  As more than 70$\%$ of reviews in the existing opinion summary data set are
positive, current opinion summarization approaches are reluctant to generate
negative summaries given the input of negative texts. To address such sentiment
bias, a direct approach without the over-reliance on a specific framework is to
generate additional data based on large language models to balance the
emotional distribution of the dataset. However, data augmentation based on
large language models faces two disadvantages: 1) the potential issues or
toxicity in the augmented data; 2) the expensive costs. Therefore, in this
paper, we propose a novel data augmentation framework based on both large and
small language models for debiasing opinion summarization. In specific, a small
size of synthesized negative reviews is obtained by rewriting the positive text
via a large language model. Then, a disentangle reconstruction model is trained
based on the generated data. After training, a large amount of synthetic data
can be obtained by decoding the new representation obtained from the
combination of different sample representations and filtering based on
confusion degree and sentiment classification. Experiments have proved that our
framework can effectively alleviate emotional bias same as using only large
models, but more economically.
\\ ( https://arxiv.org/abs/2403.07693 ,  1507kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07704 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:49:19 GMT   (13317kb,D)

Title: Symmetric Q-learning: Reducing Skewness of Bellman Error in Online
  Reinforcement Learning
Authors: Motoki Omura, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada
Categories: cs.LG cs.AI
Comments: Accepted at AAAI 2024: The 38th Annual AAAI Conference on Artificial
  Intelligence (Main Tech Track)
\\
  In deep reinforcement learning, estimating the value function to evaluate the
quality of states and actions is essential. The value function is often trained
using the least squares method, which implicitly assumes a Gaussian error
distribution. However, a recent study suggested that the error distribution for
training the value function is often skewed because of the properties of the
Bellman operator, and violates the implicit assumption of normal error
distribution in the least squares method. To address this, we proposed a method
called Symmetric Q-learning, in which the synthetic noise generated from a
zero-mean distribution is added to the target values to generate a Gaussian
error distribution. We evaluated the proposed method on continuous control
benchmark tasks in MuJoCo. It improved the sample efficiency of a
state-of-the-art reinforcement learning method by reducing the skewness of the
error distribution.
\\ ( https://arxiv.org/abs/2403.07704 ,  13317kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07708 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:51:57 GMT   (339kb,D)

Title: Improving Reinforcement Learning from Human Feedback Using Contrastive
  Rewards
Authors: Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, Yang
  Liu
Categories: cs.CL cs.AI
\\
  Reinforcement learning from human feedback (RLHF) is the mainstream paradigm
used to align large language models (LLMs) with human preferences. Yet existing
RLHF heavily relies on accurate and informative reward models, which are
vulnerable and sensitive to noise from various sources, e.g. human labeling
errors, making the pipeline fragile. In this work, we improve the effectiveness
of the reward model by introducing a penalty term on the reward, named as
\textit{contrastive rewards}. %Contrastive rewards Our approach involves two
steps: (1) an offline sampling step to obtain responses to prompts that serve
as baseline calculation and (2) a contrastive reward calculated using the
baseline responses and used in the Proximal Policy Optimization (PPO) step. We
show that contrastive rewards enable the LLM to penalize reward uncertainty,
improve robustness, encourage improvement over baselines, calibrate according
to task difficulty, and reduce variance in PPO. We show empirically contrastive
rewards can improve RLHF substantially, evaluated by both GPTs and humans, and
our method consistently outperforms strong baselines.
\\ ( https://arxiv.org/abs/2403.07708 ,  339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07711 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:53:56 GMT   (679kb,D)

Title: SSM Meets Video Diffusion Models: Efficient Video Generation with
  Structured State Spaces
Authors: Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo
Categories: cs.CV cs.AI
Comments: Accepted as workshop paper at ICLR 2024
\\
  Given the remarkable achievements in image generation through diffusion
models, the research community has shown increasing interest in extending these
models to video generation. Recent diffusion models for video generation have
predominantly utilized attention layers to extract temporal features. However,
attention layers are limited by their memory consumption, which increases
quadratically with the length of the sequence. This limitation presents
significant challenges when attempting to generate longer video sequences using
diffusion models. To overcome this challenge, we propose leveraging state-space
models (SSMs). SSMs have recently gained attention as viable alternatives due
to their linear memory consumption relative to sequence length. In the
experiments, we first evaluate our SSM-based model with UCF101, a standard
benchmark of video generation. In addition, to investigate the potential of
SSMs for longer video generation, we perform an experiment using the MineRL
Navigate dataset, varying the number of frames to 64 and 150. In these
settings, our SSM-based model can considerably save memory consumption for
longer sequences, while maintaining competitive FVD scores to the
attention-based models. Our codes are available at
https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.
\\ ( https://arxiv.org/abs/2403.07711 ,  679kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07718 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:58:45 GMT   (4544kb,D)

Title: WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work
  Tasks?
Authors: Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji,
  Manuel Del Verme, Tom Marty, L\'eo Boisvert, Megh Thakkar, Quentin Cappart,
  David Vazquez, Nicolas Chapados, Alexandre Lacoste
Categories: cs.LG cs.AI
Comments: 27 pages, 10 figures, preprint
\\
  We study the use of large language model-based agents for interacting with
software via web browsers. Unlike prior work, we focus on measuring the agents'
ability to perform tasks that span the typical daily work of knowledge workers
utilizing enterprise software systems. To this end, we propose WorkArena, a
remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow
platform. We also introduce BrowserGym, an environment for the design and
evaluation of such agents, offering a rich set of actions as well as multimodal
observations. Our empirical evaluation reveals that while current agents show
promise on WorkArena, there remains a considerable gap towards achieving full
task automation. Notably, our analysis uncovers a significant performance
disparity between open and closed-source LLMs, highlighting a critical area for
future exploration and development in the field.
\\ ( https://arxiv.org/abs/2403.07718 ,  4544kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07720 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:58:52 GMT   (1771kb,D)

Title: Multi-modal Auto-regressive Modeling via Visual Words
Authors: Tianshuo Peng, Zuchao Li, Lefei Zhang, Hai Zhao, Ping Wang, and Bo Du
Categories: cs.CV cs.AI
\\
  Large Language Models (LLMs), benefiting from the auto-regressive modelling
approach performed on massive unannotated texts corpora, demonstrates powerful
perceptual and reasoning capabilities. However, as for extending
auto-regressive modelling to multi-modal scenarios to build Large Multi-modal
Models (LMMs), there lies a great difficulty that the image information is
processed in the LMM as continuous visual embeddings, which cannot obtain
discrete supervised labels for classification. In this paper, we successfully
perform multi-modal auto-regressive modeling with a unified objective for the
first time. Specifically, we propose the concept of visual words, which maps
the visual features to probability distributions over LLM's vocabulary,
providing supervision information for visual modelling. We further explore the
distribution of visual features in the semantic space within LMM and the
possibility of using text embeddings to represent visual information.
Experimental results and ablation studies on 5 VQA tasks and 4 benchmark
toolkits validate the powerful performance of our proposed approach.
\\ ( https://arxiv.org/abs/2403.07720 ,  1771kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07724 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:01:27 GMT   (17987kb,D)

Title: Balancing Fairness and Accuracy in Data-Restricted Binary Classification
Authors: Zachary McBride Lazri, Danial Dervovic, Antigoni Polychroniadou, Ivan
  Brugere, Dana Dachman-Soled, and Min Wu
Categories: cs.LG cs.AI cs.CY stat.ML
\\
  Applications that deal with sensitive information may have restrictions
placed on the data available to a machine learning (ML) classifier. For
example, in some applications, a classifier may not have direct access to
sensitive attributes, affecting its ability to produce accurate and fair
decisions. This paper proposes a framework that models the trade-off between
accuracy and fairness under four practical scenarios that dictate the type of
data available for analysis. Prior works examine this trade-off by analyzing
the outputs of a scoring function that has been trained to implicitly learn the
underlying distribution of the feature vector, class label, and sensitive
attribute of a dataset. In contrast, our framework directly analyzes the
behavior of the optimal Bayesian classifier on this underlying distribution by
constructing a discrete approximation it from the dataset itself. This approach
enables us to formulate multiple convex optimization problems, which allow us
to answer the question: How is the accuracy of a Bayesian classifier affected
in different data restricting scenarios when constrained to be fair? Analysis
is performed on a set of fairness definitions that include group and individual
fairness. Experiments on three datasets demonstrate the utility of the proposed
framework as a tool for quantifying the trade-offs among different fairness
notions and their distributional dependencies.
\\ ( https://arxiv.org/abs/2403.07724 ,  17987kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07733 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:13:12 GMT   (27216kb,D)

Title: DSEG-LIME -- Improving Image Explanation by Hierarchical Data-Driven
  Segmentation
Authors: Patrick Knab, Sascha Marton, Christian Bartelt
Categories: cs.CV cs.AI
\\
  Explainable Artificial Intelligence is critical in unraveling decision-making
processes in complex machine learning models. LIME (Local Interpretable
Model-agnostic Explanations) is a well-known XAI framework for image analysis.
It utilizes image segmentation to create features to identify relevant areas
for classification. Consequently, poor segmentation can compromise the
consistency of the explanation and undermine the importance of the segments,
affecting the overall interpretability. Addressing these challenges, we
introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a
data-driven segmentation for human-recognized feature generation, and ii) a
hierarchical segmentation procedure through composition. We benchmark DSEG-LIME
on pre-trained models with images from the ImageNet dataset - scenarios without
domain-specific knowledge. The analysis includes a quantitative evaluation
using established XAI metrics, complemented by a qualitative assessment through
a user study. Our findings demonstrate that DSEG outperforms in most of the XAI
metrics and enhances the alignment of explanations with human-recognized
concepts, significantly improving interpretability. The code is available
under: https://github. com/patrick-knab/DSEG-LIME
\\ ( https://arxiv.org/abs/2403.07733 ,  27216kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07741 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:19:25 GMT   (2839kb,D)

Title: Uncertainty Quantification with Deep Ensembles for 6D Object Pose
  Estimation
Authors: Kira Wursthorn, Markus Hillemann, Markus Ulrich
Categories: cs.CV cs.AI
Comments: 8 pages
\\
  The estimation of 6D object poses is a fundamental task in many computer
vision applications. Particularly, in high risk scenarios such as human-robot
interaction, industrial inspection, and automation, reliable pose estimates are
crucial. In the last years, increasingly accurate and robust
deep-learning-based approaches for 6D object pose estimation have been
proposed. Many top-performing methods are not end-to-end trainable but consist
of multiple stages. In the context of deep uncertainty quantification, deep
ensembles are considered as state of the art since they have been proven to
produce well-calibrated and robust uncertainty estimates. However, deep
ensembles can only be applied to methods that can be trained end-to-end. In
this work, we propose a method to quantify the uncertainty of multi-stage 6D
object pose estimation approaches with deep ensembles. For the implementation,
we choose SurfEmb as representative, since it is one of the top-performing 6D
object pose estimation approaches in the BOP Challenge 2022. We apply
established metrics and concepts for deep uncertainty quantification to
evaluate the results. Furthermore, we propose a novel uncertainty calibration
score for regression tasks to quantify the quality of the estimated
uncertainty.
\\ ( https://arxiv.org/abs/2403.07741 ,  2839kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07743 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:22:05 GMT   (54386kb,D)

Title: Equipping Computational Pathology Systems with Artifact Processing
  Pipelines: A Showcase for Computation and Performance Trade-offs
Authors: Neel Kanwal, Farbod Khoraminia, Umay Kiraz, Andres Mosquera-Zamudio,
  Carlos Monteagudo, Emiel A.M. Janssen, Tahlita C.M. Zuiverloon, Chunmig Rong,
  and Kjersti Engan
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: Submitted to BMC Medical Informatics and Decision Making Journal
\\
  Histopathology is a gold standard for cancer diagnosis under a microscopic
examination. However, histological tissue processing procedures result in
artifacts, which are ultimately transferred to the digitized version of glass
slides, known as whole slide images (WSIs). Artifacts are diagnostically
irrelevant areas and may result in wrong deep learning (DL) algorithms
predictions. Therefore, detecting and excluding artifacts in the computational
pathology (CPATH) system is essential for reliable automated diagnosis. In this
paper, we propose a mixture of experts (MoE) scheme for detecting five notable
artifacts, including damaged tissue, blur, folded tissue, air bubbles, and
histologically irrelevant blood from WSIs. First, we train independent binary
DL models as experts to capture particular artifact morphology. Then, we
ensemble their predictions using a fusion mechanism. We apply probabilistic
thresholding over the final probability distribution to improve the sensitivity
of the MoE. We developed DL pipelines using two MoEs and two multiclass models
of state-of-the-art deep convolutional neural networks (DCNNs) and vision
transformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed
simpler multiclass models and were tested on datasets from different hospitals
and cancer types, where MoE using DCNNs yielded the best results. The proposed
MoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining
less computational cost for inference than MoE using ViTs. This best
performance of MoEs comes with relatively higher computational trade-offs than
multiclass models. The proposed artifact detection pipeline will not only
ensure reliable CPATH predictions but may also provide quality control.
\\ ( https://arxiv.org/abs/2403.07743 ,  54386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07745 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:28:21 GMT   (193kb,D)

Title: Probabilistic Easy Variational Causal Effect
Authors: Usef Faghihi and Amir Saki
Categories: stat.ML cs.AI cs.LG
Comments: 45 pages, 9 Figures
MSC-class: 26A45, 6008, 68T37, 68T20, 68T27, 68U99, 46N30, 62R10
ACM-class: G.3; I.2.3
\\
  Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one
hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the
total variation and the flux of $g$, we develop a point of view in causal
inference capable of dealing with a broad domain of causal problems. Indeed, we
focus on a function, called Probabilistic Easy Variational Causal Effect
(PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect
to continuously and interventionally changing the values of $X$ while keeping
the value of $Z$ constant. PEACE is a function of $d\ge 0$, which is a degree
managing the strengths of probability density values $f(x|z)$. On the other
hand, we generalize the above idea for the discrete case and show its
compatibility with the continuous case. Further, we investigate some properties
of PEACE using measure theoretical concepts. Furthermore, we provide some
identifiability criteria and several examples showing the generic capability of
PEACE. We note that PEACE can deal with the causal problems for which
micro-level or just macro-level changes in the value of the input variables are
important. Finally, PEACE is stable under small changes in $\partial
g_{in}/\partial x$ and the joint distribution of $X$ and $Z$, where $g_{in}$ is
obtained from $g$ by removing all functional relationships defining $X$ and
$Z$.
\\ ( https://arxiv.org/abs/2403.07745 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07747 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:32:39 GMT   (1022kb,D)

Title: FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese
  Large Language Models
Authors: Yan Liu, Renren Jin, Lin Shi, Zheng Yao, Deyi Xiong
Categories: cs.CL cs.AI
\\
  To thoroughly assess the mathematical reasoning abilities of Large Language
Models (LLMs), we need to carefully curate evaluation datasets covering diverse
mathematical concepts and mathematical problems at different difficulty levels.
In pursuit of this objective, we propose FineMath in this paper, a fine-grained
mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath
is created to cover the major key mathematical concepts taught in elementary
school math, which are further divided into 17 categories of math word
problems, enabling in-depth analysis of mathematical reasoning abilities of
LLMs. All the 17 categories of math word problems are manually annotated with
their difficulty levels according to the number of reasoning steps required to
solve these problems. We conduct extensive experiments on a wide range of LLMs
on FineMath and find that there is still considerable room for improvements in
terms of mathematical reasoning capability of Chinese LLMs. We also carry out
an in-depth analysis on the evaluation process and methods that have been
overlooked previously. These two factors significantly influence the model
results and our understanding of their mathematical reasoning capabilities. The
dataset will be publicly available soon.
\\ ( https://arxiv.org/abs/2403.07747 ,  1022kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07748 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:33:09 GMT   (29kb)

Title: Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents
  in an Unknown Graph
Authors: Romain Cosson
Categories: cs.MA cs.AI
\\
  We investigate two fundamental problems in mobile computing: exploration and
rendezvous, with two distinct mobile agents in an unknown graph. The agents can
read and write information on whiteboards that are located at all nodes. They
both move along one adjacent edge at every time-step. In the exploration
problem, both agents start from the same node of the graph and must traverse
all of its edges. We show that a simple variant of depth-first search achieves
collective exploration in $m$ synchronous time-steps, where $m$ is the number
of edges of the graph. This improves the competitive ratio of collective graph
exploration. In the rendezvous problem, the agents start from different nodes
of the graph and must meet as fast as possible. We introduce an algorithm
guaranteeing rendezvous in at most $\frac{3}{2}m$ time-steps. This improves
over the so-called `wait for Mommy' algorithm which requires $2m$ time-steps.
All our guarantees are derived from a more general asynchronous setting in
which the speeds of the agents are controlled by an adversary at all times. Our
guarantees also generalize to weighted graphs, if the number of edges $m$ is
replaced by the sum of all edge lengths.
\\ ( https://arxiv.org/abs/2403.07748 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07750 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:36:42 GMT   (5027kb,D)

Title: Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and
  Image Embeddings
Authors: Sahand Sharifzadeh, Christos Kaplanis, Shreya Pathak, Dharshan
  Kumaran, Anastasija Ilic, Jovana Mitrovic, Charles Blundell, Andrea Banino
Categories: cs.CV cs.AI
Comments: 9 pages, 6 figures
\\
  The creation of high-quality human-labeled image-caption datasets presents a
significant bottleneck in the development of Visual-Language Models (VLMs). We
propose a novel approach that leverages the strengths of Large Language Models
(LLMs) and image generation models to create synthetic image-text pairs for
efficient and effective VLM training. Our method employs pretraining a
text-to-image model to synthesize image embeddings starting from captions
generated by an LLM. These synthetic pairs are then used to train a VLM.
Extensive experiments demonstrate that the VLM trained with synthetic data
exhibits comparable performance on image captioning, while requiring a fraction
of the data used by models trained solely on human-annotated data. In
particular, we outperform the baseline by 17% through augmentation with a
synthetic dataset. Furthermore, we show that synthesizing in the image
embedding space is 25% faster than in the pixel space. This research introduces
a promising technique for generating large-scale, customizable image datasets,
leading to enhanced VLM performance and wider applicability across various
domains, all with improved data efficiency and resource utilization.
\\ ( https://arxiv.org/abs/2403.07750 ,  5027kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07788 (*cross-listing*)
Date: Tue, 12 Mar 2024 16:23:49 GMT   (17395kb,D)

Title: DexCap: Scalable and Portable Mocap Data Collection System for Dexterous
  Manipulation
Authors: Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C.
  Karen Liu
Categories: cs.RO cs.AI cs.CV cs.LG
\\
  Imitation learning from human hand motion data presents a promising avenue
for imbuing robots with human-like dexterity in real-world manipulation tasks.
Despite this potential, substantial challenges persist, particularly with the
portability of existing hand motion capture (mocap) systems and the difficulty
of translating mocap data into effective control policies. To tackle these
issues, we introduce DexCap, a portable hand motion capture system, alongside
DexIL, a novel imitation algorithm for training dexterous robot skills directly
from human hand mocap data. DexCap offers precise, occlusion-resistant tracking
of wrist and finger motions based on SLAM and electromagnetic field together
with 3D observations of the environment. Utilizing this rich dataset, DexIL
employs inverse kinematics and point cloud-based imitation learning to
replicate human actions with robot hands. Beyond learning from human motion,
DexCap also offers an optional human-in-the-loop correction mechanism to refine
and further improve robot performance. Through extensive evaluation across six
dexterous manipulation tasks, our approach not only demonstrates superior
performance but also showcases the system's capability to effectively learn
from in-the-wild mocap data, paving the way for future data collection methods
for dexterous manipulation. More details can be found at
https://dex-cap.github.io
\\ ( https://arxiv.org/abs/2403.07788 ,  17395kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07797 (*cross-listing*)
Date: Tue, 12 Mar 2024 16:34:07 GMT   (366kb,D)

Title: Joint Selection: Adaptively Incorporating Public Information for Private
  Synthetic Data
Authors: Miguel Fuentes, Brett Mullins, Ryan McKenna, Gerome Miklau, Daniel
  Sheldon
Categories: cs.LG cs.AI
\\
  Mechanisms for generating differentially private synthetic data based on
marginals and graphical models have been successful in a wide range of
settings. However, one limitation of these methods is their inability to
incorporate public data. Initializing a data generating model by pre-training
on public data has shown to improve the quality of synthetic data, but this
technique is not applicable when model structure is not determined a priori. We
develop the mechanism jam-pgm, which expands the adaptive measurements
framework to jointly select between measuring public data and private data.
This technique allows for public data to be included in a graphical-model-based
mechanism. We show that jam-pgm is able to outperform both publicly assisted
and non publicly assisted synthetic data generation mechanisms even when the
public data distribution is biased.
\\ ( https://arxiv.org/abs/2403.07797 ,  366kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07805 (*cross-listing*)
Date: Tue, 12 Mar 2024 16:42:44 GMT   (8762kb,D)

Title: Beyond Memorization: The Challenge of Random Memory Access in Language
  Models
Authors: Tongyao Zhu, Qian Liu, Liang Pang, Zhengbao Jiang, Min-Yen Kan, Min
  Lin
Categories: cs.CL cs.AI
Comments: 8 pages, 4 figures
\\
  Recent developments in Language Models (LMs) have shown their effectiveness
in NLP tasks, particularly in knowledge-intensive tasks. However, the
mechanisms underlying knowledge storage and memory access within their
parameters remain elusive. In this paper, we investigate whether a generative
LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through
carefully-designed synthetic tasks, covering the scenarios of full recitation,
selective recitation and grounded question answering, we reveal that LMs manage
to sequentially access their memory while encountering challenges in randomly
accessing memorized content. We find that techniques including recitation and
permutation improve the random memory access capability of LMs. Furthermore, by
applying this intervention to realistic scenarios of open-domain question
answering, we validate that enhancing random access by recitation leads to
notable improvements in question answering. The code to reproduce our
experiments can be found at https://github.
com/sail-sg/lm-random-memory-access.
\\ ( https://arxiv.org/abs/2403.07805 ,  8762kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07815 (*cross-listing*)
Date: Tue, 12 Mar 2024 16:53:54 GMT   (1128kb,D)

Title: Chronos: Learning the Language of Time Series
Authors: Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro
  Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian
  Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Michael
  W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider,
  Yuyang Wang
Categories: cs.LG cs.AI
Comments: Inference code and model checkpoints available at
  https://github.com/amazon-science/chronos-forecasting
\\
  We introduce Chronos, a simple yet effective framework for pretrained
probabilistic time series models. Chronos tokenizes time series values using
scaling and quantization into a fixed vocabulary and trains existing
transformer-based language model architectures on these tokenized time series
via the cross-entropy loss. We pretrained Chronos models based on the T5 family
(ranging from 20M to 710M parameters) on a large collection of publicly
available datasets, complemented by a synthetic dataset that we generated via
Gaussian processes to improve generalization. In a comprehensive benchmark
consisting of 42 datasets, and comprising both classical local models and deep
learning methods, we show that Chronos models: (a) significantly outperform
other methods on datasets that were part of the training corpus; and (b) have
comparable and occasionally superior zero-shot performance on new datasets,
relative to methods that were trained specifically on them. Our results
demonstrate that Chronos models can leverage time series data from diverse
domains to improve zero-shot accuracy on unseen forecasting tasks, positioning
pretrained models as a viable tool to greatly simplify forecasting pipelines.
\\ ( https://arxiv.org/abs/2403.07815 ,  1128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07816 (*cross-listing*)
Date: Tue, 12 Mar 2024 16:54:58 GMT   (946kb,D)

Title: Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM
Authors: Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria
  Lin, Baptiste Rozi\`ere, Jacob Kahn, Daniel Li, Wen-tau Yih, Jason Weston,
  Xian Li
Categories: cs.CL cs.AI
\\
  We investigate efficient methods for training Large Language Models (LLMs) to
possess capabilities in multiple specialized domains, such as coding, math
reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts
from a seed model, which is branched to train experts in embarrassingly
parallel fashion with high throughput and reduced communication cost. After
individual experts are asynchronously trained, BTX brings together their
feedforward parameters as experts in Mixture-of-Expert (MoE) layers and
averages the remaining parameters, followed by an MoE-finetuning stage to learn
token-level routing. BTX generalizes two special cases, the Branch-Train-Merge
method, which does not have the MoE finetuning stage to learn routing, and
sparse upcycling, which omits the stage of training experts asynchronously.
Compared to alternative approaches, BTX achieves the best accuracy-efficiency
tradeoff.
\\ ( https://arxiv.org/abs/2403.07816 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07818 (*cross-listing*)
Date: Tue, 12 Mar 2024 16:57:56 GMT   (1419kb,D)

Title: Label Dropout: Improved Deep Learning Echocardiography Segmentation
  Using Multiple Datasets With Domain Shift and Partial Labelling
Authors: Iman Islam (1), Esther Puyol-Ant\'on (1), Bram Ruijsink (1), Andrew J.
  Reader (1), Andrew P. King (1) ((1) King's College London)
Categories: cs.CV cs.AI cs.LG
Comments: 10 pages, 5 figures, submitted to MICCAI conference
\\
  Echocardiography (echo) is the first imaging modality used when assessing
cardiac function. The measurement of functional biomarkers from echo relies
upon the segmentation of cardiac structures and deep learning models have been
proposed to automate the segmentation process. However, in order to translate
these tools to widespread clinical use it is important that the segmentation
models are robust to a wide variety of images (e.g. acquired from different
scanners, by operators with different levels of expertise etc.). To achieve
this level of robustness it is necessary that the models are trained with
multiple diverse datasets. A significant challenge faced when training with
multiple diverse datasets is the variation in label presence, i.e. the combined
data are often partially-labelled. Adaptations of the cross entropy loss
function have been proposed to deal with partially labelled data. In this paper
we show that training naively with such a loss function and multiple diverse
datasets can lead to a form of shortcut learning, where the model associates
label presence with domain characteristics, leading to a drop in performance.
To address this problem, we propose a novel label dropout scheme to break the
link between domain characteristics and the presence or absence of labels. We
demonstrate that label dropout improves echo segmentation Dice score by 62% and
25% on two cardiac structures when training using multiple diverse partially
labelled datasets.
\\ ( https://arxiv.org/abs/2403.07818 ,  1419kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07839 (*cross-listing*)
Date: Tue, 12 Mar 2024 17:24:26 GMT   (3099kb,D)

Title: MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with
  Module-wise Pruning Error Metric
Authors: Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying
  Wei, Zhenan Sun
Categories: cs.CV cs.AI cs.MM
Comments: 18 pages, 8 figures, Published in CVPR2024
Journal-ref: In Proc. IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2024
\\
  Vision-language pre-trained models have achieved impressive performance on
various downstream tasks. However, their large model sizes hinder their
utilization on platforms with limited computational resources. We find that
directly using smaller pre-trained models and applying magnitude-based pruning
on CLIP models leads to inflexibility and inferior performance. Recent efforts
for VLP compression either adopt uni-modal compression metrics resulting in
limited performance or involve costly mask-search processes with learnable
masks. In this paper, we first propose the Module-wise Pruning Error (MoPE)
metric, accurately assessing CLIP module importance by performance decline on
cross-modal tasks. Using the MoPE metric, we introduce a unified pruning
framework applicable to both pre-training and task-specific fine-tuning
compression stages. For pre-training, MoPE-CLIP effectively leverages knowledge
from the teacher model, significantly reducing pre-training costs while
maintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning
from width to depth yields highly competitive task-specific models. Extensive
experiments in two stages demonstrate the effectiveness of the MoPE metric, and
MoPE-CLIP outperforms previous state-of-the-art VLP compression methods.
\\ ( https://arxiv.org/abs/2403.07839 ,  3099kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07865 (*cross-listing*)
Date: Tue, 12 Mar 2024 17:55:38 GMT   (8284kb,D)

Title: Exploring Safety Generalization Challenges of Large Language Models via
  Code
Authors: Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam,
  Lizhuang Ma
Categories: cs.CL cs.AI cs.CR cs.LG cs.SE
\\
  The rapid advancement of Large Language Models (LLMs) has brought about
remarkable capabilities in natural language processing but also raised concerns
about their potential misuse. While strategies like supervised fine-tuning and
reinforcement learning from human feedback have enhanced their safety, these
methods primarily focus on natural languages, which may not generalize to other
domains. This paper introduces CodeAttack, a framework that transforms natural
language inputs into code inputs, presenting a novel environment for testing
the safety generalization of LLMs. Our comprehensive studies on
state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a
common safety vulnerability of these models against code input: CodeAttack
consistently bypasses the safety guardrails of all models more than 80\% of the
time. Furthermore, we find that a larger distribution gap between CodeAttack
and natural language leads to weaker safety generalization, such as encoding
natural language input with data structures or using less popular programming
languages. These findings highlight new safety risks in the code domain and the
need for more robust safety alignment algorithms to match the code capabilities
of LLMs.
\\ ( https://arxiv.org/abs/2403.07865 ,  8284kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07869 (*cross-listing*)
Date: Tue, 12 Mar 2024 17:58:01 GMT   (2772kb,D)

Title: TeleMoMa: A Modular and Versatile Teleoperation System for Mobile
  Manipulation
Authors: Shivin Dass, Wensi Ai, Yuqian Jiang, Samik Singh, Jiaheng Hu, Ruohan
  Zhang, Peter Stone, Ben Abbatematteo, Roberto Martin-Martin
Categories: cs.RO cs.AI cs.LG
\\
  A critical bottleneck limiting imitation learning in robotics is the lack of
data. This problem is more severe in mobile manipulation, where collecting
demonstrations is harder than in stationary manipulation due to the lack of
available and easy-to-use teleoperation interfaces. In this work, we
demonstrate TeleMoMa, a general and modular interface for whole-body
teleoperation of mobile manipulators. TeleMoMa unifies multiple human
interfaces including RGB and depth cameras, virtual reality controllers,
keyboard, joysticks, etc., and any combination thereof. In its more accessible
version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering
the entry bar for humans to provide mobile manipulation demonstrations. We
demonstrate the versatility of TeleMoMa by teleoperating several existing
mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and
the real world. We demonstrate the quality of the demonstrations collected with
TeleMoMa by training imitation learning policies for mobile manipulation tasks
involving synchronized whole-body motion. Finally, we also show that TeleMoMa's
teleoperation channel enables teleoperation on site, looking at the robot, or
remote, sending commands and observations through a computer network, and
perform user studies to evaluate how easy it is for novice users to learn to
collect demonstrations with different combinations of human interfaces enabled
by our system. We hope TeleMoMa becomes a helpful tool for the community
enabling researchers to collect whole-body mobile manipulation demonstrations.
For more information and video results,
https://robin-lab.cs.utexas.edu/telemoma-web.
\\ ( https://arxiv.org/abs/2403.07869 ,  2772kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2209.00953
replaced with revised version Tue, 12 Mar 2024 02:43:40 GMT   (629kb,D)

Title: SATformer: Transformer-Based UNSAT Core Learning
Authors: Zhengyuan Shi (1), Min Li (1), Yi Liu (1), Sadaf Khan (1), Junhua
  Huang (2), Hui-Ling Zhen (2), Mingxuan Yuan (2), Qiang Xu (1) ((1) The
  Chinese University of Hong Kong, (2) Huawei Noah's Ark Lab)
Categories: cs.AI cs.LG cs.LO
Journal-ref: In 2023 IEEE/ACM International Conference on Computer Aided Design
  (ICCAD) 2023 Oct 28 (pp. 1-4). IEEE
DOI: 10.1109/ICCAD57390.2023.10323731
\\ ( https://arxiv.org/abs/2209.00953 ,  629kb)
------------------------------------------------------------------------------
\\
arXiv:2302.09582
replaced with revised version Tue, 12 Mar 2024 14:55:29 GMT   (6341kb)

Title: Language-Specific Representation of Emotion-Concept Knowledge Causally
  Supports Emotion Inference
Authors: Ming Li, Yusheng Su, Hsiu-Yuan Huang, Jiali Cheng, Xin Hu, Xinmiao
  Zhang, Huadong Wang, Yujia Qin, Xiaozhi Wang, Kristen A. Lindquist, Zhiyuan
  Liu, Dan Zhang
Categories: cs.AI cs.CL
Comments: 44 pages, 14 figures, 2 tables
\\ ( https://arxiv.org/abs/2302.09582 ,  6341kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11432
replaced with revised version Tue, 12 Mar 2024 09:51:35 GMT   (4914kb,D)

Title: A Survey on Large Language Model based Autonomous Agents
Authors: Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and
  Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and
  Wayne Xin Zhao and Zhewei Wei and Ji-Rong Wen
Categories: cs.AI cs.CL
Comments: 35 pages, 5 figures, 3 tables, has been accepted by frontiers of
  computer science (FCS), doi={10.1007/s11704-024-40231-1}
\\ ( https://arxiv.org/abs/2308.11432 ,  4914kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14363
replaced with revised version Tue, 12 Mar 2024 02:17:03 GMT   (9017kb,D)

Title: Mobile Foundation Model as Firmware
Authors: Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan, Zeling
  Zhang, Xiang Li, Dingge Zhang, Hanzi Mei, Xianqing Jia, Shangguang Wang,
  Mengwei Xu
Categories: cs.AI
Comments: 17 pages, 15 figures, published to ACM MobiCom'24
Journal-ref: The 30th Annual International Conference on Mobile Computing and
  Networking, 2024
DOI: 10.1145/3636534.3649361
\\ ( https://arxiv.org/abs/2308.14363 ,  9017kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02099
replaced with revised version Tue, 12 Mar 2024 16:13:59 GMT   (5216kb,D)

Title: A Safe Preference Learning Approach for Personalization with
  Applications to Autonomous Vehicles
Authors: Ruya Karagulle and Nikos Arechiga and Andrew Best and Jonathan
  DeCastro and Necmiye Ozay
Categories: cs.AI cs.SY eess.SY
Comments: 9 pages, 3 figures, 2 tables. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible
\\ ( https://arxiv.org/abs/2311.02099 ,  5216kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12403
replaced with revised version Tue, 12 Mar 2024 10:09:50 GMT   (40kb,D)

Title: On Alternating-Time Temporal Logic, Hyperproperties, and Strategy
  Sharing
Authors: Raven Beutner, Bernd Finkbeiner
Categories: cs.AI cs.LO cs.MA
Comments: AAAI 2024
\\ ( https://arxiv.org/abs/2312.12403 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10568
replaced with revised version Tue, 12 Mar 2024 08:24:37 GMT   (10017kb,D)

Title: CivRealm: A Learning and Reasoning Odyssey in Civilization for
  Decision-Making Agents
Authors: Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang, Bangcheng
  Yang, Pring Wong, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu, Wei
  Wang, Yaodong Yang, Song-Chun Zhu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.10568 ,  10017kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05359
replaced with revised version Mon, 11 Mar 2024 23:15:10 GMT   (585kb,D)

Title: Guiding Large Language Models with Divide-and-Conquer Program for
  Discerning Problem Solving
Authors: Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu
Categories: cs.AI cs.CL cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2402.05359 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10011
replaced with revised version Tue, 12 Mar 2024 12:38:09 GMT   (164kb,D)

Title: Clifford Group Equivariant Simplicial Message Passing Networks
Authors: Cong Liu, David Ruhe, Floor Eijkelboom, Patrick Forr\'e
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.10011 ,  164kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18679
replaced with revised version Tue, 12 Mar 2024 17:26:53 GMT   (12998kb,D)

Title: Data Interpreter: An LLM Agent For Data Science
Authors: Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang
  Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang,
  Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang,
  Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu,
  Chenglin Wu
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.18679 ,  12998kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02760
replaced with revised version Tue, 12 Mar 2024 11:29:07 GMT   (397kb)

Title: Emerging Synergies Between Large Language Models and Machine Learning in
  Ecommerce Recommendations
Authors: Xiaonan Xu, Yichao Wu, Penghao Liang, Yuhang He, Han Wang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.02760 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04964
replaced with revised version Mon, 11 Mar 2024 18:41:29 GMT   (290kb)

Title: Tell me the truth: A system to measure the trustworthiness of Large
  Language Models
Authors: Carlo Lipizzi
Categories: cs.AI cs.CL cs.CY
\\ ( https://arxiv.org/abs/2403.04964 ,  290kb)
------------------------------------------------------------------------------
\\
arXiv:2104.12582
replaced with revised version Mon, 11 Mar 2024 18:58:33 GMT   (35kb)

Title: Understanding and Avoiding AI Failures: A Practical Guide
Authors: Heather M. Williams, Roman V. Yampolskiy
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2104.12582 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2108.00408
replaced with revised version Mon, 11 Mar 2024 23:20:07 GMT   (785kb)

Title: CSC-Unet: A Novel Convolutional Sparse Coding Strategy Based Neural
  Network for Semantic Segmentation
Authors: Haitong Tang, Shuang He, Mengduo Yang, Xia Lu, Qin Yu, Kaiyue Liu,
  Hongjie Yan and Nizhuan Wang
Categories: cs.CV cs.AI cs.LG
Journal-ref: IEEE Access,2024
DOI: 10.1109/ACCESS.2024.3373619
\\ ( https://arxiv.org/abs/2108.00408 ,  785kb)
------------------------------------------------------------------------------
\\
arXiv:2109.00031
replaced with revised version Mon, 11 Mar 2024 18:11:50 GMT   (7124kb)

Title: Deep DNA Storage: Scalable and Robust DNA Storage via Coding Theory and
  Deep Learning
Authors: Daniella Bar-Lev, Itai Orr, Omer Sabary, Tuvi Etzion, Eitan Yaakobi
Categories: cs.IT cs.AI math.IT
\\ ( https://arxiv.org/abs/2109.00031 ,  7124kb)
------------------------------------------------------------------------------
\\
arXiv:2111.13800
replaced with revised version Tue, 12 Mar 2024 17:25:35 GMT   (1239kb,D)

Title: A Two-Stage Feature Selection Approach for Robust Evaluation of
  Treatment Effects in High-Dimensional Observational Data
Authors: Md Saiful Islam, Sahil Shikalgar, Md. Noor-E-Alam
Categories: cs.LG cs.AI stat.ME
\\ ( https://arxiv.org/abs/2111.13800 ,  1239kb)
------------------------------------------------------------------------------
\\
arXiv:2201.13224
replaced with revised version Tue, 12 Mar 2024 15:46:43 GMT   (1007kb,D)

Title: Evaluating a Methodology for Increasing AI Transparency: A Case Study
Authors: David Piorkowski, John Richards, Michael Hind
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2201.13224 ,  1007kb)
------------------------------------------------------------------------------
\\
arXiv:2202.03580
replaced with revised version Tue, 12 Mar 2024 09:27:38 GMT   (182kb,D)

Title: Convolutional Neural Networks on Graphs with Chebyshev Approximation,
  Revisited
Authors: Mingguo He, Zhewei Wei, Ji-Rong Wen
Categories: cs.LG cs.AI
Comments: NeurIPS 2022
\\ ( https://arxiv.org/abs/2202.03580 ,  182kb)
------------------------------------------------------------------------------
\\
arXiv:2203.02158 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 13:57:45 GMT   (1109kb,D)

Title: Transformations in Learned Image Compression from a Modulation
  Perspective
Authors: Youneng Bao, Fangyang Meng, Wen Tan, Chao Li, Yonghong Tian and
  Yongsheng Liang
Categories: eess.IV cs.AI cs.CV
Comments: 10 pages, 8 figures
\\ ( https://arxiv.org/abs/2203.02158 ,  1109kb)
------------------------------------------------------------------------------
\\
arXiv:2210.09292
replaced with revised version Tue, 12 Mar 2024 02:08:37 GMT   (3889kb,D)

Title: Efficient Diffusion Models for Vision: A Survey
Authors: Anwaar Ulhaq and Naveed Akhtar
Categories: cs.CV cs.AI
Comments: 14 Pages, 5 Figures (in progress)
\\ ( https://arxiv.org/abs/2210.09292 ,  3889kb)
------------------------------------------------------------------------------
\\
arXiv:2211.02695
replaced with revised version Tue, 12 Mar 2024 15:12:55 GMT   (1757kb,D)

Title: WaveNets: Wavelet Channel Attention Networks
Authors: Hadi Salman, Caleb Parks, Shi Yin Hong, Justin Zhan
Categories: cs.CV cs.AI
Comments: IEEE BigData2022 conference
DOI: 10.1109/BigData55660.2022.10020665
\\ ( https://arxiv.org/abs/2211.02695 ,  1757kb)
------------------------------------------------------------------------------
\\
arXiv:2211.04118
replaced with revised version Tue, 12 Mar 2024 08:29:41 GMT   (1052kb,D)

Title: ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning
Authors: Jinta Weng and Yifan Deng and d Donghao Li and Hao You and Yue Hu and
  Heyan Huang
Categories: cs.CL cs.AI
Comments: 2 figures
Journal-ref: ICASSP2024
\\ ( https://arxiv.org/abs/2211.04118 ,  1052kb)
------------------------------------------------------------------------------
\\
arXiv:2211.14611
replaced with revised version Tue, 12 Mar 2024 16:07:12 GMT   (1158kb)

Title: The Principles of Data-Centric AI (DCAI)
Authors: Mohammad Hossein Jarrahi, Ali Memariani, Shion Guha
Categories: cs.LG cs.AI cs.HC
ACM-class: E.0; I.2
Journal-ref: Communications of the ACM (2023)
DOI: 10.1145/3571724
\\ ( https://arxiv.org/abs/2211.14611 ,  1158kb)
------------------------------------------------------------------------------
\\
arXiv:2303.08115
replaced with revised version Mon, 11 Mar 2024 22:08:41 GMT   (2774kb,D)

Title: Human-Inspired Framework to Accelerate Reinforcement Learning
Authors: Ali Beikmohammadi and Sindri Magn\'usson
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2303.08115 ,  2774kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13040
replaced with revised version Tue, 12 Mar 2024 08:52:02 GMT   (8341kb,D)

Title: SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented
  Dialogue Agents
Authors: Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei
  Dai, Hangyu Li, Rui Yan, Fei Huang, Yongbin Li
Categories: cs.CL cs.AI
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2305.13040 ,  8341kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14550
replaced with revised version Mon, 11 Mar 2024 21:22:22 GMT   (768kb,D)

Title: When should we prefer Decision Transformers for Offline Reinforcement
  Learning?
Authors: Prajjwal Bhargava, Rohan Chitnis, Alborz Geramifard, Shagun Sodhani,
  Amy Zhang
Categories: cs.LG cs.AI
Comments: International Conference on Learning Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2305.14550 ,  768kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03403
replaced with revised version Tue, 12 Mar 2024 12:41:04 GMT   (19721kb,D)

Title: SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic
  Segmentation
Authors: Xuewei Li, Tao Wu, Zhongang Qi, Gaoang Wang, Ying Shan, Xi Li
Categories: cs.CV cs.AI cs.LG cs.MM
Comments: Accepted by IJCAI 2023
\\ ( https://arxiv.org/abs/2306.03403 ,  19721kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08543
replaced with revised version Tue, 12 Mar 2024 16:15:19 GMT   (309kb,D)

Title: Knowledge Distillation of Large Language Models
Authors: Yuxian Gu, Li Dong, Furu Wei, Minlie Huang
Categories: cs.CL cs.AI
Comments: Published as a conference paper in ICLR 2024
\\ ( https://arxiv.org/abs/2306.08543 ,  309kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08762
replaced with revised version Mon, 11 Mar 2024 19:13:04 GMT   (110kb,D)

Title: Theoretical Hardness and Tractability of POMDPs in RL with Partial
  Online State Information
Authors: Ming Shi, Yingbin Liang, and Ness Shroff
Categories: cs.LG cs.AI
Comments: Submitted for publication
\\ ( https://arxiv.org/abs/2306.08762 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14114
replaced with revised version Tue, 12 Mar 2024 12:39:03 GMT   (1632kb,D)

Title: TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning
  Granger Causal Structure from Event Sequences
Authors: Yuequn Liu, Ruichu Cai, Wei Chen, Jie Qiao, Yuguang Yan, Zijian Li,
  Keli Zhang, Zhifeng Hao
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2306.14114 ,  1632kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00543
replaced with revised version Tue, 12 Mar 2024 13:44:55 GMT   (11075kb,D)

Title: Defending Against Poisoning Attacks in Federated Learning with
  Blockchain
Authors: Nanqing Dong, Zhipeng Wang, Jiahao Sun, Michael Kampffmeyer, William
  Knottenbelt, Eric Xing
Categories: cs.LG cs.AI cs.CR cs.GT
Comments: Accepted by IEEE Transactions on Artificial Intelligence
\\ ( https://arxiv.org/abs/2307.00543 ,  11075kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09476
replaced with revised version Tue, 12 Mar 2024 07:00:02 GMT   (487kb,D)

Title: Overthinking the Truth: Understanding how Language Models Process False
  Demonstrations
Authors: Danny Halawi, Jean-Stanislas Denain, Jacob Steinhardt
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2307.09476 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16140
replaced with revised version Tue, 12 Mar 2024 07:23:51 GMT   (17457kb,D)

Title: Fully $1\times1$ Convolutional Network for Lightweight Image
  Super-Resolution
Authors: Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu
Categories: cs.CV cs.AI
Comments: Accepted by Machine Intelligence Research, DOI:
  10.1007/s11633-024-1401-z
DOI: 10.1007/s11633-024-1401-z
\\ ( https://arxiv.org/abs/2307.16140 ,  17457kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02409 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 12:25:24 GMT   (1542kb,D)

Title: Mental Workload Estimation with Electroencephalogram Signals by
  Combining Multi-Space Deep Models
Authors: Hong-Hai Nguyen, Ngumimi Karen Iyortsuun, Seungwon Kim, Hyung-Jeong
  Yang, and Soo-Hyung Kim
Categories: eess.SP cs.AI cs.LG
Comments: 16 pages, 5 figures
\\ ( https://arxiv.org/abs/2308.02409 ,  1542kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04522
replaced with revised version Tue, 12 Mar 2024 00:16:38 GMT   (6291kb,D)

Title: Deep Learning for Steganalysis of Diverse Data Types: A review of
  methods, taxonomy, challenges and future directions
Authors: Hamza Kheddar, Mustapha Hemis, Yassine Himeur, David Meg\'ias, Abbes
  Amira
Categories: cs.CR cs.AI cs.LG cs.MM cs.SD eess.AS eess.IV
Journal-ref: Neurocomputing, Elsevier, 2024
DOI: 10.1016/j.neucom.2024.127528
\\ ( https://arxiv.org/abs/2308.04522 ,  6291kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13212
replaced with revised version Tue, 12 Mar 2024 07:49:44 GMT   (3219kb,D)

Title: SEGNO: Generalizing Equivariant Graph Neural Networks with Physical
  Inductive Biases
Authors: Yang Liu, Jiashun Cheng, Haihong Zhao, Tingyang Xu, Peilin Zhao, Fugee
  Tsung, Jia Li, Yu Rong
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.13212 ,  3219kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00770
replaced with revised version Tue, 12 Mar 2024 00:50:00 GMT   (826kb,D)

Title: Bias and Fairness in Large Language Models: A Survey
Authors: Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim,
  Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed
Categories: cs.CL cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2309.00770 ,  826kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02001
replaced with revised version Mon, 11 Mar 2024 18:31:09 GMT   (52kb,D)

Title: Analyzing domain shift when using additional data for the MICCAI KiTS23
  Challenge
Authors: George Stoica, Mihaela Breaban and Vlad Barbu
Categories: cs.CV cs.AI cs.LG
Comments: This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in https://link.springer.com/book/10.1007/978-3-031-54806-2, and is
  available online at https://doi.org/10.1007/978-3-031-54806-2_4
Journal-ref: Kidney and Kidney Tumor Segmentation. KiTS 2023. Lecture Notes in
  Computer Science, vol 14540
DOI: 10.1007/978-3-031-54806-2_4
\\ ( https://arxiv.org/abs/2309.02001 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10691
replaced with revised version Tue, 12 Mar 2024 15:53:06 GMT   (1415kb,D)

Title: MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language
  Feedback
Authors: Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao
  Peng, Heng Ji
Categories: cs.CL cs.AI cs.LG
Comments: ICLR 2024. Code is available on our project website:
  https://xingyaoww.github.io/mint-bench
\\ ( https://arxiv.org/abs/2309.10691 ,  1415kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16319
replaced with revised version Tue, 12 Mar 2024 03:43:57 GMT   (1553kb,D)

Title: Augmenting Transformers with Recursively Composed Multi-grained
  Representations
Authors: Xiang Hu, Qingyang Zhu, Kewei Tu, Wei Wu
Categories: cs.CL cs.AI
Comments: ICLR 2024 poster
\\ ( https://arxiv.org/abs/2309.16319 ,  1553kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02129
replaced with revised version Tue, 12 Mar 2024 16:58:53 GMT   (447kb,D)

Title: Unveiling the Pitfalls of Knowledge Editing for Large Language Models
Authors: Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
Categories: cs.CL cs.AI cs.CV cs.DB cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.02129 ,  447kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06117
replaced with revised version Tue, 12 Mar 2024 04:38:27 GMT   (763kb,D)

Title: Take a Step Back: Evoking Reasoning via Abstraction in Large Language
  Models
Authors: Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed
  H. Chi, Quoc V Le and Denny Zhou
Categories: cs.LG cs.AI cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.06117 ,  763kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18511
replaced with revised version Tue, 12 Mar 2024 11:52:42 GMT   (23149kb,D)

Title: 3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for
  Compositional Recognition
Authors: Habib Slim, Xiang Li, Yuchen Li, Mahmoud Ahmed, Mohamed Ayman, Ujjwal
  Upadhyay, Ahmed Abdelreheem, Arpit Prajapati, Suhail Pothigara, Peter Wonka,
  Mohamed Elhoseiny
Categories: cs.CV cs.AI
Comments: https://3dcompat-dataset.org/v2/
\\ ( https://arxiv.org/abs/2310.18511 ,  23149kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11227
replaced with revised version Tue, 12 Mar 2024 05:26:45 GMT   (4554kb,D)

Title: FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the
  Power of Heterogeneous Clients
Authors: Shangchao Su, Bin Li, Xiangyang Xue
Categories: cs.LG cs.AI cs.DC
\\ ( https://arxiv.org/abs/2311.11227 ,  4554kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16480
replaced with revised version Tue, 12 Mar 2024 12:07:39 GMT   (2749kb,D)

Title: WsiCaption: Multiple Instance Generation of Pathology Reports for
  Gigapixel Whole-Slide Images
Authors: Pingyi Chen, Honglin Li, Chenglu Zhu, Sunyi Zheng, Zhongyi Shui, Lin
  Yang
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2311.16480 ,  2749kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03781
replaced with revised version Tue, 12 Mar 2024 08:13:01 GMT   (41069kb,D)

Title: Lite-Mind: Towards Efficient and Robust Brain Representation Network
Authors: Zixuan Gong, Qi Zhang, Duoqian Miao, Guangyin Bao, Liang Hu
Categories: cs.CV cs.AI
Comments: 17 pages
\\ ( https://arxiv.org/abs/2312.03781 ,  41069kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09982
replaced with revised version Mon, 11 Mar 2024 19:24:41 GMT   (1672kb,D)

Title: ACPO: AI-Enabled Compiler-Driven Program Optimization
Authors: Amir H. Ashouri, Muhammad Asif Manzoor, Duc Minh Vu, Raymond Zhang,
  Ziwen Wang, Angel Zhang, Bryan Chan, Tomasz S. Czajkowski and Yaoqing Gao
Categories: cs.PL cs.AI cs.LG cs.PF
Comments: Preprint version of ACPO (12 pages)
ACM-class: I.2.5; D.3.0; I.2.6
\\ ( https://arxiv.org/abs/2312.09982 ,  1672kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15020
replaced with revised version Tue, 12 Mar 2024 07:12:38 GMT   (1881kb,D)

Title: Automated Approaches to Detect Self-Admitted Technical Debt: A
  Systematic Literature Review
Authors: Edi Sutoyo, Andrea Capiluppi
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2312.15020 ,  1881kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16145
replaced with revised version Mon, 11 Mar 2024 18:13:54 GMT   (48209kb,D)

Title: One-Dimensional Adapter to Rule Them All: Concepts, Diffusion Models and
  Erasing Applications
Authors: Mengyao Lyu, Yuhong Yang, Haiwen Hong, Hui Chen, Xuan Jin, Yuan He,
  Hui Xue, Jungong Han, Guiguang Ding
Categories: cs.CV cs.AI cs.LG
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2312.16145 ,  48209kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11624
replaced with revised version Tue, 12 Mar 2024 04:38:53 GMT   (183kb,D)

Title: In-context Learning with Retrieved Demonstrations for Language Models: A
  Survey
Authors: Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2401.11624 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03227
replaced with revised version Tue, 12 Mar 2024 11:28:20 GMT   (3069kb,D)

Title: IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of
  brain MR images
Authors: Vincent Roca, Gr\'egory Kuchcinski, Jean-Pierre Pruvo, Dorian
  Manouvriez, Renaud Lopes
Categories: cs.CV cs.AI cs.LG
Comments: 23 pages, 8 figures; typos corrected
\\ ( https://arxiv.org/abs/2402.03227 ,  3069kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08156
replaced with revised version Tue, 12 Mar 2024 17:37:26 GMT   (150kb)

Title: Group Decision-Making among Privacy-Aware Agents
Authors: Marios Papachristou, M. Amin Rahimian
Categories: cs.LG cs.AI cs.CR cs.MA stat.ML
\\ ( https://arxiv.org/abs/2402.08156 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09430 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 11:48:02 GMT   (11432kb,D)

Title: WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing
Authors: Shuokang Huang, Kaihan Li, Di You, Yichong Chen, Arvin Lin, Siying
  Liu, Xiaohui Li, Julie A. McCann
Categories: eess.SP cs.AI cs.CV cs.MM
Comments: We present WiMANS, to our knowledge, the first dataset for multi-user
  activity sensing based on WiFi
\\ ( https://arxiv.org/abs/2402.09430 ,  11432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09442 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 11:14:15 GMT   (574kb)

Title: Progress in artificial intelligence applications based on the
  combination of self-driven sensors and deep learning
Authors: Weixiang Wan, Wenjian Sun, Qiang Zeng, Linying Pan, Jingyu Xu, Bo Liu
Categories: eess.SP cs.AI
Comments: This aticle was accepted by ieee conference
\\ ( https://arxiv.org/abs/2402.09442 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09786
replaced with revised version Tue, 12 Mar 2024 13:36:23 GMT   (29299kb,D)

Title: Examining Pathological Bias in a Generative Adversarial Network
  Discriminator: A Case Study on a StyleGAN3 Model
Authors: Alvin Grissom II, Ryan F. Lei, Matt Gusdorff, Jeova Farias Sales Rocha
  Neto, Bailey Lin, Ryan Trotter
Categories: cs.CV cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2402.09786 ,  29299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10885
replaced with revised version Mon, 11 Mar 2024 22:05:00 GMT   (28406kb,D)

Title: 3D Diffuser Actor: Policy Diffusion with 3D Scene Representations
Authors: Tsung-Wei Ke, Nikolaos Gkanatsios, Katerina Fragkiadaki
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: First two authors contributed equally
\\ ( https://arxiv.org/abs/2402.10885 ,  28406kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12177
replaced with revised version Tue, 12 Mar 2024 16:04:23 GMT   (60kb)

Title: Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning
Authors: Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.12177 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13254
replaced with revised version Tue, 12 Mar 2024 17:59:56 GMT   (10188kb,D)

Title: CounterCurate: Enhancing Physical and Semantic Visio-Linguistic
  Compositional Reasoning via Counterfactual Examples
Authors: Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: 13 pages, 6 figures, 8 tables, Project Page:
  https://countercurate.github.io/
\\ ( https://arxiv.org/abs/2402.13254 ,  10188kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15441
replaced with revised version Tue, 12 Mar 2024 07:44:00 GMT   (1249kb,D)

Title: Active Few-Shot Fine-Tuning
Authors: Jonas H\"ubotter and Bhavya Sukhija and Lenart Treven and Yarden As
  and Andreas Krause
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.15441 ,  1249kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15898
replaced with revised version Tue, 12 Mar 2024 07:37:03 GMT   (2457kb,D)

Title: Information-based Transductive Active Learning
Authors: Jonas H\"ubotter, Bhavya Sukhija, Lenart Treven, Yarden As, Andreas
  Krause
Categories: cs.LG cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2402.15441
\\ ( https://arxiv.org/abs/2402.15898 ,  2457kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17257
replaced with revised version Tue, 12 Mar 2024 04:48:46 GMT   (25025kb,D)

Title: RIME: Robust Preference-based Reinforcement Learning with Noisy
  Preferences
Authors: Jie Cheng, Gang Xiong, Xingyuan Dai, Qinghai Miao, Yisheng Lv, Fei-Yue
  Wang
Categories: cs.LG cs.AI cs.RO
\\ ( https://arxiv.org/abs/2402.17257 ,  25025kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18603
replaced with revised version Tue, 12 Mar 2024 16:35:25 GMT   (946kb,D)

Title: MMSR: Symbolic Regression is a Multimodal Task
Authors: Yanjie Li, Jingyi Liu, Weijun Li, Lina Yu, Min Wu, Wenqiang Li, Meilan
  Hao, Su Wei, Yusong Deng
Categories: cs.LG cs.AI cs.CL
Comments: 12 page
\\ ( https://arxiv.org/abs/2402.18603 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01548
replaced with revised version Tue, 12 Mar 2024 09:49:28 GMT   (1339kb,D)

Title: In-Context Sharpness as Alerts: An Inner Representation Perspective for
  Hallucination Mitigation
Authors: Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang
  Gao, Junxian He
Categories: cs.CL cs.AI cs.LG
Comments: code repo is available at:
  https://github.com/hkust-nlp/Activation_decoding.git
\\ ( https://arxiv.org/abs/2403.01548 ,  1339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02624
replaced with revised version Tue, 12 Mar 2024 06:28:39 GMT   (11944kb,D)

Title: Pareto-Optimal Estimation and Policy Learning on Short-term and
  Long-term Treatment Effects
Authors: Yingrong Wang, Anpeng Wu, Haoxuan Li, Weiming Liu, Qiaowei Miao,
  Ruoxuan Xiong, Fei Wu, Kun Kuang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.02624 ,  11944kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03102
replaced with revised version Tue, 12 Mar 2024 05:33:16 GMT   (11338kb,D)

Title: "In Dialogues We Learn": Towards Personalized Dialogue Without
  Pre-defined Profiles through In-Dialogue Learning
Authors: Chuanqi Cheng, Quan Tu, Wei Wu, Shuo Shang, Cunli Mao, Zhengtao Yu,
  Rui Yan
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.03102 ,  11338kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04558
replaced with revised version Tue, 12 Mar 2024 11:42:06 GMT   (1167kb,D)

Title: Reducing self-supervised learning complexity improves weakly-supervised
  classification performance in computational pathology
Authors: Tim Lenz, Omar S. M. El Nahhas, Marta Ligero, Jakob Nikolas Kather
Categories: cs.LG cs.AI cs.CV
Comments: Submitted to MICCAI 2024
\\ ( https://arxiv.org/abs/2403.04558 ,  1167kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05326
replaced with revised version Tue, 12 Mar 2024 12:12:36 GMT   (1927kb,D)

Title: ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in
  Dialogues
Authors: Yiding Liu and Jingjing Wang and Jiamin Luo and Tao Zeng and Guodong
  Zhou
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.05326 ,  1927kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05527
replaced with revised version Mon, 11 Mar 2024 18:55:40 GMT   (2567kb,D)

Title: GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
  Generative Inference of LLM
Authors: Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu,
  Tushar Krishna, Tuo Zhao
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2403.05527 ,  2567kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05918
replaced with revised version Tue, 12 Mar 2024 02:45:48 GMT   (2409kb)

Title: SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to
  Imbalanced Data
Authors: Ming Zheng, Yang Yang, Zhi-Hang Zhao, Shan-Chao Gan, Yang Chen, Si-Kai
  Ni and Yang Lu
Categories: cs.LG cs.AI
Comments: None
\\ ( https://arxiv.org/abs/2403.05918 ,  2409kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06025
replaced with revised version Tue, 12 Mar 2024 17:35:29 GMT   (13284kb,D)

Title: CarbonNet: How Computer Vision Plays a Role in Climate Change?
  Application: Learning Geomechanics from Subsurface Geometry of CCS to
  Mitigate Global Warming
Authors: Wei Chen, Yunan Li and Yuan Tian
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.06025 ,  13284kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06054 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 03:22:13 GMT   (13981kb,D)

Title: Decoupled Data Consistency with Diffusion Purification for Image
  Restoration
Authors: Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishanka, Qing
  Qu
Categories: eess.IV cs.AI cs.CV cs.LG eess.SP
\\ ( https://arxiv.org/abs/2403.06054 ,  13981kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06397
replaced with revised version Tue, 12 Mar 2024 02:13:51 GMT   (4693kb,D)

Title: DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe
  Multi-Agent Reinforcement Learning
Authors: Xuefeng Wang, Henglin Pu, Hyung Jun Kim and Husheng Li
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: 8 pages, 5 figures
\\ ( https://arxiv.org/abs/2403.06397 ,  4693kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06670
replaced with revised version Tue, 12 Mar 2024 03:04:15 GMT   (6526kb,D)

Title: CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar
  Class-Incremental Learning
Authors: Xinyuan Gao, Songlin Dong, Yuhang He, Xing Wei, Yihong Gong
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.06670 ,  6526kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06914
replaced with revised version Tue, 12 Mar 2024 15:52:14 GMT   (339kb,D)

Title: MEND: Meta dEmonstratioN Distillation for Efficient and Effective
  In-Context Learning
Authors: Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo
Categories: cs.CL cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2403.06914 ,  339kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
